<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>jotaro shigeyama</title>
    <link>https://jotaros.github.io/index.xml</link>
    <description>Recent content on jotaro shigeyama</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Jotaro Shigeyama</copyright>
    <lastBuildDate>Tue, 12 Jan 2021 19:34:11 +0100</lastBuildDate>
    <atom:link href="https://jotaros.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Unityで対話型音声認識アプリを作れるフレームワークをつくった</title>
      <link>https://jotaros.github.io/post/speechioforunity/</link>
      <pubDate>Tue, 12 Jan 2021 19:34:11 +0100</pubDate>
      
      <guid>https://jotaros.github.io/post/speechioforunity/</guid>
      <description>

&lt;blockquote&gt;
&lt;p&gt;macOS, Windows両方で動く、Unityで対話型の音声認識アプリがすぐに作れるフレームワークを作りました。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Downloads&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;リポジトリへのリンク：&lt;a href=&#34;https://github.com/HassoPlattnerInstituteHCI/SpeechIOForUnity&#34; target=&#34;_blank&#34;&gt;https://github.com/HassoPlattnerInstituteHCI/SpeechIOForUnity&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Unity Package: &lt;a href=&#34;https://github.com/HassoPlattnerInstituteHCI/SpeechIOForUnity/releases/tag/v1.0&#34; target=&#34;_blank&#34;&gt;https://github.com/HassoPlattnerInstituteHCI/SpeechIOForUnity/releases/tag/v1.0&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;つかいかた&#34;&gt;つかいかた&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;&lt;code&gt;SpeechOut/SpeechIn&lt;/code&gt;を宣言し、以下のように書くだけ。&lt;/strong&gt;
&lt;pre&gt;
&lt;code class=&#34;language-cs&#34;&gt;
SpeechOut speechOut = new SpeechOut();
SpeechIn  speechIn  = new SpeechIn(OnRecognized);&lt;/p&gt;

&lt;p&gt;void Start(){
    Dialog();
}&lt;/p&gt;

&lt;p&gt;async void Dialog(){
    await speechOut.Speak(&amp;ldquo;Hello!&amp;rdquo;);
    await speechIn.Listen(new string[] { &amp;ldquo;Hello&amp;rdquo;, &amp;ldquo;Hi&amp;rdquo;, &amp;ldquo;Hey&amp;rdquo; });
    await speechOut.Speak(&amp;ldquo;How are you doing?&amp;rdquo;);
    await speechIn.Listen(new string[] { &amp;ldquo;I&amp;rsquo;m fine&amp;rdquo;, &amp;ldquo;Nah&amp;rdquo;, &amp;ldquo;I&amp;rsquo;m Sick&amp;rdquo; });&lt;/p&gt;

&lt;p&gt;}
&lt;/code&gt;
&lt;/pre&gt;&lt;/p&gt;

&lt;h2 id=&#34;なぜつくったか&#34;&gt;なぜつくったか&lt;/h2&gt;

&lt;p&gt;macOS, Windowsを使う学生のために、Unityを扱う授業で音声認識を使った対話型アプリを作るためのフレームワークがあればいいなと思ったので作りました。
主に非同期処理の&lt;code&gt;async/await&lt;/code&gt;ベースで、逐次処理をわかりやすく書けるように、また速度や”間”などのチューニングも簡単にできるようにしました。&lt;/p&gt;

&lt;p&gt;授業ではmac, Windowsそれぞれを使う学生を同様に対応しないといけないのですが、Unityそのものは双方で動作する一方、ネイティブOSの機能を使おうとすると相互にコードを共有できない＝他のチームやグループが作ったアプリを動作させられないことになるので、このようなコードを作りました。&lt;/p&gt;

&lt;p&gt;授業のためとはいえ、一般的に使いやすいツールだと思います。&lt;/p&gt;

&lt;h2 id=&#34;osネイティブの音声認識-音声合成&#34;&gt;OSネイティブの音声認識・音声合成&lt;/h2&gt;

&lt;h3 id=&#34;macos側&#34;&gt;macOS側&lt;/h3&gt;

&lt;p&gt;macOSネイティブのオフライン音声認識ツールとしては、&lt;code&gt;NSSpeechRecognizer&lt;/code&gt;が、音声合成ツールとしては、コマンドラインツールとして&lt;code&gt;say&lt;/code&gt;があります。このうち、&lt;code&gt;say&lt;/code&gt;に関してはUnity上の&lt;code&gt;System.Diagnostics.Process&lt;/code&gt;から呼び出すことで実行できます（この点、Argsをいじるだけで声質とか速度を変えられるのでサイコー）&lt;/p&gt;

&lt;p&gt;一方の&lt;code&gt;NSSpeechRecognizer&lt;/code&gt;はOSネイティブAPIとして提供されている機能なので、実行するためには&lt;code&gt;Objective-C&lt;/code&gt;もしくは&lt;code&gt;Swift&lt;/code&gt;で記述されたコードから実行する必要があります。&lt;/p&gt;

&lt;p&gt;今回のフレームワークでは別途&lt;a href=&#34;https://github.com/HassoPlattnerInstituteHCI/SpeechIOForUnity/tree/master/NSSpeechForUnity&#34; target=&#34;_blank&#34;&gt;NSSpeechForUnity&lt;/a&gt;として、&lt;code&gt;NSSpeechRecognizer&lt;/code&gt;を呼び出す&lt;code&gt;Objective-C&lt;/code&gt;コードを書き、外部ライブラリとして&lt;code&gt;.bundle&lt;/code&gt;ファイルを書き出し、それをUnityのPluginとすることで実行しています。&lt;/p&gt;

&lt;h3 id=&#34;windows側&#34;&gt;Windows側&lt;/h3&gt;

&lt;p&gt;逆にWindows側では&lt;code&gt;UnityEngine.Windows.Speech&lt;/code&gt;なるモジュールがあり、音声認識に関してはUnityから直接実行できる一方、音声合成に関しては別途WindowsのネイティブAPI＝&lt;a href=&#34;https://ja.wikipedia.org/wiki/Speech_Application_Programming_Interface&#34; target=&#34;_blank&#34;&gt;SAPI&lt;/a&gt;を叩く必要がありました。コード内で別途&lt;a href=&#34;https://github.com/HassoPlattnerInstituteHCI/SpeechIOForUnity/tree/master/WindowsVoiceProject&#34; target=&#34;_blank&#34;&gt;WindowsVoiceProject&lt;/a&gt;として、Visual Studioから.dllをビルドできるプロジェクトを作り、その.dllをUnityのPluginにします。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;以上２つのOSに関して独自にライブラリを書き出すことによって実現しましたが、Unity上で非同期の動作を実現させるため、それぞれの処理がきちんと終わったかどうかを常に監視するためのコードを書く必要があります。たとえば&lt;a href=&#34;https://github.com/HassoPlattnerInstituteHCI/SpeechIOForUnity/blob/0ac6cfe8b468f970524b6dc558d0656bf501322c/NSSpeechForUnity/NSSpeechForUnity/native.m#L75&#34; target=&#34;_blank&#34;&gt;このように&lt;/a&gt;、内部のStateを逐一変えて、Unity側から監視することで、UnityのAsync/Awaitが進行するように若干HardCodedな感じではありますが、リアルタイムアプリケーションのための非同期処理を実現しています。&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;応用&#34;&gt;応用&lt;/h2&gt;

&lt;p&gt;対話型アプリケーションのためとは言いましたが、VRアプリケーションなどのための目と手が離せない際のデバッグ（イベントが起こったときにどのフラグが立ったか喋って教えてくれる）や、視覚障害者向けアプリなど、様々な用途が考えられます。「デバッグのときになにか喋ってくれたら便利だな」とか、「アクセシビリティ機能の充実したアプリを作りたいな」などというときにはぜひ使ってみてください。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SpeechIOForUnity</title>
      <link>https://jotaros.github.io/project/speechioforunity/</link>
      <pubDate>Fri, 28 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://jotaros.github.io/project/speechioforunity/</guid>
      <description>

&lt;p&gt;A ready-to-use Unity plugin for Speech Input/Output using native Speech API of &lt;i&gt;both&lt;/i&gt; Apple macOS and MS Windows.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/HassoPlattnerInstituteHCI/SpeechIOForUnity&#34; target=&#34;_blank&#34;&gt;github project page is here.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Author: Jotaro Shigeyama and Thijs Roumen&lt;/p&gt;

&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Custom command speech input&lt;/li&gt;
&lt;li&gt;Support for various speed / language.&lt;/li&gt;
&lt;li&gt;Async/await-based interaction design.&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-example.cs&#34;&gt;
SpeechOut speechOut = new SpeechOut();
SpeechIn  speechIn  = new SpeechIn(OnRecognized);

void Start(){
    Dialog();
}

async void Dialog(){
    await speechOut.Speak(&amp;quot;Hello!&amp;quot;);
    await speechIn.Listen(new string[] { &amp;quot;Hello&amp;quot;, &amp;quot;Hi&amp;quot;, &amp;quot;Hey&amp;quot; });
    await speechOut.Speak(&amp;quot;How are you doing?&amp;quot;);
    await speechIn.Listen(new string[] { &amp;quot;I&#39;m fine&amp;quot;, &amp;quot;Nah&amp;quot;, &amp;quot;I&#39;m Sick&amp;quot; });
    //...
}

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This project repo contains&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Unity project (this repo, tested with v.2019.3.0a8)&lt;/li&gt;
&lt;li&gt;XCode project NSSpeechForUnity (author: Jotaro Shigeyama)&lt;/li&gt;
&lt;li&gt;Visual Studio project WindowsVoiceProject originally from here (&lt;a href=&#34;https://chadweisshaar.com/blog/2015/07/02/microsoft-speech-for-unity/&#34; target=&#34;_blank&#34;&gt;https://chadweisshaar.com/blog/2015/07/02/microsoft-speech-for-unity/&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Unity project contains pre-built &lt;code&gt;.dll&lt;/code&gt; and &lt;code&gt;.bundle&lt;/code&gt; from above source project.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;installation&#34;&gt;Installation&lt;/h2&gt;

&lt;p&gt;This plugin works and tested on macOS Catalina or above, and Windows 10 (Windows 8 is not supported).&lt;/p&gt;

&lt;h3 id=&#34;os-setup&#34;&gt;OS setup&lt;/h3&gt;

&lt;p&gt;Right now English / Dutch / German / Japanese are supported. You need to install necessary language module from your OS setting.&lt;/p&gt;

&lt;h3 id=&#34;unity&#34;&gt;Unity&lt;/h3&gt;

&lt;p&gt;Just simply grab all Scripts, Plugins (and if Scenes if you need) to your own Unity project file.&lt;/p&gt;

&lt;h3 id=&#34;potential-installation-issue-for-macos&#34;&gt;Potential installation issue for macOS&lt;/h3&gt;

&lt;p&gt;Some macOS users will experience broken speech input due to missing dictation kits: mostly because of bug on Apple. If you encounter some issue on speech input, please try these.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;make sure your native voice command system works (System Preferences &amp;gt; Accessibility &amp;gt; Voice Command, enabling voice command will invoke macOS system voice command input windows.)

&lt;ul&gt;
&lt;li&gt;Go to &amp;ldquo;Preferences &amp;gt; Accessibility &amp;gt; Sound Control &amp;gt; Language&amp;rdquo; and install language pack.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;In case of macOS have some buggy issues: Try switching your OS language to another, then try to install your desired voice command module (macOS will prompt you to download missing dictation model.)&lt;/li&gt;
&lt;li&gt;Make sure your macOS is the latest version&lt;/li&gt;
&lt;li&gt;Try rebooting the system.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;dev-installation&#34;&gt;dev-Installation&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;XCode (mac)&lt;/li&gt;
&lt;li&gt;Visual Studio (win, latest Windows SDK required)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;testing-your-nsspeechrecognizer&#34;&gt;Testing your NSSpeechRecognizer&lt;/h3&gt;

&lt;p&gt;tested on Apple Swift version 5.1.3&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd NSSpeechTest
swift run
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;using-in-your-own-unity-package&#34;&gt;Using in your own Unity package&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Build NSSpeechForUnity in XCode.&lt;/li&gt;
&lt;li&gt;Copy generated &lt;code&gt;.bundle&lt;/code&gt; file in &lt;code&gt;Assets/Plugins&lt;/code&gt; of your Unity project.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;modifying-voice-command-dictionary&#34;&gt;Modifying voice command dictionary&lt;/h3&gt;

&lt;p&gt;See &lt;code&gt;MyMacSpeechScript.cs&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;documentation&#34;&gt;Documentation&lt;/h2&gt;

&lt;p&gt;coming soon.&lt;/p&gt;

&lt;p&gt;Your feedback is welcome!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kerf-Canceling Mechanisms: Making Laser-Cut Mechanisms Operate Across Different Laser Cutters</title>
      <link>https://jotaros.github.io/project/kerfc/</link>
      <pubDate>Thu, 20 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://jotaros.github.io/project/kerfc/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;https://jotaros.github.io/img/kerfc.png&#34; alt=&#34;Example image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br&gt;
&lt;center&gt; Kerf-Canceling Mechanisms: Making Laser-Cut Mechanisms Operate Across Different Laser Cutters&lt;/center&gt;&lt;/p&gt;

&lt;h4 id=&#34;abstract&#34;&gt;Abstract&lt;/h4&gt;

&lt;p&gt;Getting laser-cut mechanisms, such as those in microscopes,robots, vehicles, etc., to work, requires all their components to be dimensioned precisely. This precision, however, tends to be lost when fabricating on a different laser cutter, as it is likely to remove more or less material (aka kerf). We address this with what we call kerf-canceling mechanisms. Kerf-canceling mechanisms replace laser-cut bearings, sliders, gear pairs, etc. Unlike their traditional counterparts, however, they keep working when manufactured on a different laser cutter and/or with different kerf. Kerf-canceling mechanisms achieve this by adding an additional wedge element per mechanism. We have created a software tool Kerf-Canceler that locates traditional mechanisms in cutting plans and replaces them with their kerf-canceling counterparts.&lt;/p&gt;

&lt;p&gt;For more details, paper and video, visit &lt;a href=&#34;https://hpi.de/baudisch/projects/kerf-canceling-mechanisms.html&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;UIST2020 Technical Paper&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Kerf-Canceling Mechanisms: Making Laser-Cut Mechanisms Operate Across Different Laser Cutters&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MetaなVirtualは，Virtual Conferenceを救うか？</title>
      <link>https://jotaros.github.io/post/meta-virtual/</link>
      <pubDate>Sat, 25 Apr 2020 00:00:00 +0100</pubDate>
      
      <guid>https://jotaros.github.io/post/meta-virtual/</guid>
      <description>

&lt;h2 id=&#34;学会は-永遠に姿を変えた&#34;&gt;「学会は, 永遠に姿を変えた」?&lt;/h2&gt;

&lt;p&gt;COVID-19の影響で，世界各国・様々な分野の学会が中止・もしくはオンライン開催となっている．
VRやHCI分野でのオンサイト学会中止は &lt;u&gt;特にデモの発表が必須な触覚・タンジブル関係の分野に致命的な打撃を与えかねない&lt;/u&gt;&lt;/p&gt;

&lt;p&gt;オンラインでデモを開催するとなったとして，物を触っている際の絶妙な体験を離れた人のために伝えるためにはどういった工夫が必要なのか．例えば，「指先にこんな力が加わっていますよ」ということを示すための矢印(ベクトル?)，みたいなものもあるかもしれないが，個人的には情報可視化の域を超えない気がする(なぜ？→ここでのゴールは体験の可視化だから)&lt;/p&gt;

&lt;p&gt;「白猫プロジェクト」というコロプラ社の開発したスマホゲームに，「ぷにコン」という疑似触覚を用いたインタフェースが採用されている（一時期，任天堂の権利を侵害したとして広く話題になった→DSのゲームで同様のインタフェースがあった．厳密には異なると思うけど・・・）．「あたかもぷにぷにした触覚を触っているように感じさせる」視覚的刺激＝疑似触覚（Pseudo-haptics)を利用したものだが，&lt;u&gt;このインタフェースは実はゲーム配信をした際にも，視聴者に同様に触体験が届く&lt;/u&gt; ．&lt;/p&gt;

&lt;h2 id=&#34;meta-virtual&#34;&gt;Meta-Virtual&lt;/h2&gt;

&lt;p&gt;&lt;u&gt;バーチャルリアリティの永遠の課題は「あたかも◯◯したように感じさせられる」体験の設計&lt;/u&gt; であり，それはそのままそっくり，Virtualの定義を与える．学会でのバーチャルリアリティ・デモは，実際に学会会場でHMDをかぶってもらって，あたかも◯◯したかのように感じてもらって，自らの研究の意義を伝えてきた．しかし今後数年間は，思うようにデモをできなくなるだろうし，今後永遠に学会そのものの形が変わってしまうことも考えられる（環境問題もしかり）．&lt;/p&gt;

&lt;p&gt;つまり，あるバーチャルな体験を，リモートでさらに体験したかのように感じられる，&lt;u&gt;メタ・バーチャル&lt;/u&gt; が必要，ということになるのではないか．そういう意味で「ぷにコン」はよくできているなぁと思う．今，YoutubeにはUSJやら鼠の王国のライドをそのまま動画にしたものまでも転がっており，再生数を無駄に稼ぎ，コンテンツとして消費され，ゴミの山と化している．「目で見て消費されるだけの体験」と，「体験として心に残り続ける，メタ・バーチャル」との差別化が求められている・・・気がする．&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>UIST2019参加報告</title>
      <link>https://jotaros.github.io/post/uist2019-report/</link>
      <pubDate>Thu, 30 Jan 2020 00:00:00 +0100</pubDate>
      
      <guid>https://jotaros.github.io/post/uist2019-report/</guid>
      <description>

&lt;blockquote&gt;
&lt;p&gt;本記事はVR学会の記事 (&lt;a href=&#34;https://vrsj.org/report/10864/&#34; target=&#34;_blank&#34;&gt;https://vrsj.org/report/10864/&lt;/a&gt;) に掲載されたものの本人による抜粋です．&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ユーザインタフェースに関するトップカンファレンスの一つであるUIST2019(&lt;a href=&#34;https://uist.acm.org/uist2019/)が，アメリカ・ルイジアナ州最大の都市であるニューオリンズで10月20日から23日の4日間の日程にて開催された．1988年から始まったUISTは今回で32回目を数えた．Paper&#34; target=&#34;_blank&#34;&gt;https://uist.acm.org/uist2019/)が，アメリカ・ルイジアナ州最大の都市であるニューオリンズで10月20日から23日の4日間の日程にて開催された．1988年から始まったUISTは今回で32回目を数えた．Paper&lt;/a&gt; trackは全381件のSubmissionに対して93件が採択され，採択率は24%であった．前回のベルリンでのUIST2018(&lt;sup&gt;80&lt;/sup&gt;&amp;frasl;&lt;sub&gt;375&lt;/sub&gt; = 21%)よりも多く，東京でのUIST2016(&lt;sup&gt;79&lt;/sup&gt;&amp;frasl;&lt;sub&gt;384&lt;/sub&gt;=21%)に次いで2位のSubmission数となり，投稿数はここ１０年間で上昇傾向にある．　筆者は本学会のOrganizerとして昨年に引き続きRegistration Chairを務めた．今年のUISTは510人の参加があり，前回ベルリンの560人に対して減少しており，学会の開催地に参加者数が影響されると考えられる．学生の参加はうち254人であ　り，参加者全体の半数以上を占めた．日本からの参加者は73名で，アメリカ(270名)についで2位であった．国際的にも日本のユーザインタフェース研究の規模は大きいと考えられる．学会会場はニューオリンズ市内中心部のフレンチ・クオーターにあるRoyal Sonesta Hotelで開催された．ジャズ発祥の地とあって，朝早くからジャズの生演奏があちらこちらで聞こえていた．&lt;/p&gt;

&lt;h3 id=&#34;面白かった研究とか&#34;&gt;面白かった研究とか&lt;/h3&gt;

&lt;p&gt;今回のPaperセッションは2セッションが並行して進められる形式であった．VR関連のセッションも複数あり，AR/MR，ハプティクス，VRヘッドセット技術に関して27件と，全体のPaperの中でもVRが最も多いキーワードとなった．個人的に興味深い研究としては，Lindlbauerらによるユーザの認知負荷に応じてリアルタイムにMR空間内のGUIの情報量を最適化するシステム，Marweckiらの視線トラッキングを用いてVR内の視界外の物体表示を操作する研究，Heoらの振動感覚だけで棒を曲げる・伸ばすなどの感覚が得られるデバイスなどがあった．　また，Sinclairらによるキャプスタンをもちいて超小型モータのトルクを増幅しつつ極めて応答性の高い力覚デバイスや，Jeらによるドローンのロータを用いたVRコントローラなど，触覚ハードウェアに関する研究も多く紹介された．筆者のものも含め，近年様々なVR触覚提示デバイスが提案されていたが，個人的な考えとしては，デバイスの新規性や性能にとどまらず，むしろそれらを用いて如何に体験をデザインするかという議論をすべきではないかと感じさせられた．Demoセッションではそれらの触覚デバイスをはじめ様々なシステムを実際に触れて体験できる機会が設けられた．&lt;/p&gt;

&lt;h3 id=&#34;面白かったセッションやら&#34;&gt;面白かったセッションやら&lt;/h3&gt;

&lt;p&gt;3日目の夕方には，UISTの参加者に向けて今後のユーザインタフェースのVisionを語る，UIST Visionsというセッションが開催された．セッションではUniversitéParis-SudからMichel Beaudouin-Lafon教授がアプリというソフトウェア同士の垣根の概念を超えた存在について”A World Without Apps”というテーマで，また東京大学から暦本純一教授がヒトとAIの融合を通じた人間拡張について”Homo Cyberneticus: The Era of Human-AI Integration”というテーマで発表した．学生を含む参加者からの質疑応答もあり，それぞれのテーマについて活発な議論がなされた．UIST Visionsは前回に引き続いて2回目の開催であった．本学会にラディカルな議論の場が設けられたことから，コミュニティ全体のユーザインタフェース研究における新たなブレークスルーへの需要がさらに高まっているのではないかと感じた．　学会のBanquetは会場からほど近いミシシッピ川沿いのAudubon Aquariumを貸し切って開催された．Student Innovation ContestもBanquet中に開催され，Edge TPUなどを用いた学生らの作品群が多くの参加者の注目を集めていた．Student Innovation Contestは学部生などにもコミュニティへの参加を促す目的もあり，場合によっては旅費・参加費が支援されるため，日本のUI研究に興味がある学生にはぜひ積極的に参加してほしいと感じた．&lt;/p&gt;

&lt;h3 id=&#34;uist-is-about-earth&#34;&gt;UIST is about Earth&lt;/h3&gt;

&lt;p&gt;本学会ではSustainabilityをテーマとして，学会開催に伴う環境問題に考慮した学会運営がなされた．具体的には，カンファレンスで配られるバッグやプログラムを配布しない・紙とリサイクル可能な素材のみで作ったカンファレンスバッジなどの対策が取られた．国際的に環境問題への関心が高まる中で，航空機の移動など伴う国際会議がもたらす影響が非常に大きいことを鑑みれば，今後VRカンファレンスなどの需要が極めて高まると考えられる．Closing KeynoteにおいてもVRカンファレンスに関するプレゼンがあり，今回のUISTでも実験的にVR会場が設けられるなどされたが，VRカンファレンスに関してはノウハウが足りていない部分も見受けられる．日本のVR研究者がVRカンファレンスの重要性について更に議論し，ぜひ今後のVR学会大会ではVRカンファレンスも実践し，分野を世界的にリードする役割を担うべきだろう．　33回目となる次回のUIST2020(&lt;a href=&#34;https://uist.acm.org/uist2020/)は，2020年10月20日-23日に，アメリカ・ミネソタ州のミネアポリスで開催される予定である．&#34; target=&#34;_blank&#34;&gt;https://uist.acm.org/uist2020/)は，2020年10月20日-23日に，アメリカ・ミネソタ州のミネアポリスで開催される予定である．&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>springFit: Joints and Mounts that Fabricate on Any Laser-Cutter</title>
      <link>https://jotaros.github.io/project/springfit/</link>
      <pubDate>Thu, 01 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://jotaros.github.io/project/springfit/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;https://jotaros.github.io/img/springfit.png&#34; alt=&#34;springfit&#34; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;abstract&#34;&gt;Abstract&lt;/h3&gt;

&lt;p&gt;Joints are crucial to laser cutting as they allow making three-dimensional objects; mounts are crucial because they allow embedding technical components, such as motors. Unfortunately, mounts and joints tend to fail when trying to fabricate a model on a different laser cutter or from a different material. The reason for this lies in the way mounts and joints hold objects in place, which is by forcing them into slightly smaller openings. Such “press fit” mechanisms unfortunately are susceptible to the small changes in diameter that occur when switching to a machine that removes more or less material (“kerf”), as well as to changes in stiffness, as they occur when switching to a different material.&lt;/p&gt;

&lt;p&gt;We present a software tool called springFit that resolves this problem by replacing the problematic press fit-based mounts and joints with what we call cantileverbased mounts and joints. A cantilever spring is simply a long thin piece of material that pushes against the object to be held. Unlike press fits, cantilever springs are robust against variations in kerf and material; they can even handle very high variations, simply by using longer springs. SpringFit converts models in the form of 2D cutting plans by replacing all contained mounts, notch joints, finger joints, and t-joints.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;jotaro-s-contribution&#34;&gt;Jotaro&amp;rsquo;s contribution&lt;/h3&gt;

&lt;p&gt;In this project, Jotaro has contributed to the system that automatically generates ideal parameters of the spring geometry: cantilver spring that produces aimed forces and deflection and will not break easily. Based on the &lt;em&gt;Euler-Bernoulli cantilver theory&lt;/em&gt;, Jotaro has build C++ executable system that optimizes the spring geometry based on given geometric constraints.&lt;/p&gt;

&lt;p&gt;For more details, paper and video, visit &lt;a href=&#34;https://hpi.de/baudisch/projects/springfit.html&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;publication&#34;&gt;Publication&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Thijs Roumen, &lt;u&gt;&lt;strong&gt;Jotaro Shigeyama&lt;/strong&gt;&lt;/u&gt;, Julius Romeo Cosmo Rudolph, Felix Grzelka, and Patrick Baudisch: SpringFit: Joints and Mounts that Fabricate on Any Laser Cutter, In Proceedings of UIST&amp;rsquo;19, October, 2019, New Orleans, LA, US.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Project Transcalibur</title>
      <link>https://jotaros.github.io/project/transcalibur/</link>
      <pubDate>Fri, 01 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://jotaros.github.io/project/transcalibur/</guid>
      <description>

&lt;h3 id=&#34;downloads&#34;&gt;Downloads:&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#39;https://jotaros.github.io/transcalibur/shigeyama-preprint.pdf&#39;&gt;Preprint Paper (PDF 7MB)&lt;/a&gt;  //   &lt;a href=&#39;https://jotaros.github.io/transcalibur/transcalibur-movie.mp4&#39;&gt; Movie (MP3 20MB)&lt;/a&gt;   //     &lt;a href=&#39;https://jotaros.github.io/transcalibur/presskit.zip&#39;&gt; Press Kit (ZIP Archive 20MB)&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;


&lt;div style=&#34;position: relative; padding-bottom: 56.25%; padding-top: 30px; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;//www.youtube.com/embed/OiSbn6D5kwA?&#34; 
  style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%;&#34; allowfullscreen frameborder=&#34;0&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br&gt;
&lt;center&gt; Transcalibur: A Weight Shifting Virtual Reality Controller&lt;br&gt; for 2D Shape Rendering based on Computational Perception Model&lt;/center&gt;&lt;/p&gt;

&lt;h4 id=&#34;abstract&#34;&gt;Abstract&lt;/h4&gt;

&lt;p&gt;Humans can estimate the shape of a wielded object through the illusory feeling of the mass properties of the object obtained using their hands. Even though the shape of hand-held objects influences immersion and realism in virtual reality (VR), it is difficult to design VR controllers for rendering desired shapes according to the perceptions derived from the illusory effects of mass properties and shape perception. We propose Transcalibur, which is a hand-held VR controller that can render a 2D shape by changing its mass properties on a 2D planar area. We built a computational perception model using a data-driven approach from the collected data pairs of mass properties and perceived shapes. This enables Transcalibur to easily and effectively provide convincing shape perception based on complex illusory effects. Our user study showed that the system succeeded in providing the perception of various desired shapes in a virtual environment.&lt;/p&gt;

&lt;h4 id=&#34;award&#34;&gt;Award:&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#39;https://jotaros.github.io/img/hm.png&#39; style=&#39;width:30px; min-height:30px; margin-bottom:0px;&#39;&gt; This paper has been awarded a Best Paper Honorable Mention at CHI2019.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&#39;https://jotaros.github.io/img/transcalibur-teaser.png&#39; width=&#39;100%&#39;&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&#34;hardware&#34;&gt;Hardware&lt;/h4&gt;

&lt;p&gt;The weight moving mechanism is designed to move along the 2D planner space and to be &lt;em&gt;non-backdrivable&lt;/em&gt;. The &lt;em&gt;angular mechanism&lt;/em&gt; enables to rotate two arms, and &lt;em&gt;weight mechanisms&lt;/em&gt; enable to shift the position of the weight module independently.&lt;/p&gt;

&lt;p&gt;&lt;center&gt; &lt;img class=&#39;half&#39; src=&#39;https://jotaros.github.io/img/transform.gif&#39;&gt; &lt;img class=&#39;half&#39; src=&#39;https://jotaros.github.io/img/transcalibur-hardware.png&#39;&gt;&lt;/center&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&#34;computational-perception-model&#34;&gt;Computational Perception Model&lt;/h4&gt;

&lt;p&gt;Based on the object shown in VR, the shape of the controller that users grasp is dynamically changed so that its shape perception matches the target object.
In our system, we create a computational model that maps mass properties to haptically perceived shape using a data-driven approach.
We correct the perceived shape data of the VR controller with different mass properties through a perceptual experiment and map these data using regression.
From the model, we determine the mass properties of the VR controller that optimizes the perceived shape of the controller to be the target object shown in virtual environment (VE).
In this manner, we can easily and efficiently render an arbitrary 2D shape through the controller.&lt;/p&gt;

&lt;p&gt;We assumed a perception model $f$ that maps the physical configuration of the controller $\phi$ to the perceived shape of the wielded object in VR $\psi$:&lt;/p&gt;

&lt;p&gt;$$
f: \phi \mapsto \psi
$$&lt;/p&gt;

&lt;p&gt;&lt;img src=&#39;https://jotaros.github.io/img/transcalibur-approach.png&#39;&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&#34;experiments-results&#34;&gt;Experiments / Results&lt;/h4&gt;

&lt;p&gt;In the data collection experiment, we provided the participants with various shapes of the controller and asked them to report the perceived shapes in VE.
This generates matched pairs of $(\phi_i,\psi_i)$, which are used to build a regression model for the training data. Using the obtained data pairs, we performed regression analysis to build a map $f$ from the configurations of the controller onto the perceived shapes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#39;https://jotaros.github.io/img/dataCollection.jpg&#39;&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;To measure the validity of the perception model, we conducted validation experiments. The ten virtual shapes were manually determined such that variations in height, width, symmetricity, and asymmetricity of the target shapes could be evaluated. Overall, our perception model succeeded in providing various target shapes in VR for Transcalibur, leaving a few shapes with confusions on shapes 4 and 5 or 8 and 9.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#39;https://jotaros.github.io/img/transcalibur-outputlist.png&#39;&gt;&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;img class=&#39;half&#39; src=&#39;https://jotaros.github.io/img/transcalibur-confusionmat.png&#39;&gt;&lt;/center&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h4&gt;

&lt;p&gt;In this paper, we introduced Transcalibur: the weight moving VR controller for 2D haptic shape illusion.
We implemented a hardware prototype, which can change its mass property in 2D planar space, and applied data-driven methods to obtain maps between mass property and perceived shape.
Based on the demonstration and experiment, we succeeded in rendering various shape perceptions through the controller based on pre-computed perception model.
As a future work, we further investigate details on time factor of shape changing in VR, and we aim to develop a simpler design and yet maximizes range of rendering shape.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;CHI2019 Technical Paper&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Transcalibur: A Weight Shifting Virtual Reality Controller for 2D Shape Rendering based on Computational Perception Model&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;h4 id=&#34;authors&#34;&gt;Authors:&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://jotaros.github.io&#34; target=&#34;_blank&#34;&gt;Jotaro Shigeyama&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://takeruace.github.io/&#34; target=&#34;_blank&#34;&gt;Takeru Hashimoto&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.shigeodayo.com&#34; target=&#34;_blank&#34;&gt;Shigeo Yoshida&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cyber.t.u-tokyo.ac.jp/~narumi/&#34; target=&#34;_blank&#34;&gt;Takuji Narumi&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cyber.t.u-tokyo.ac.jp/&#34; target=&#34;_blank&#34;&gt;Tomohiro Tanikawa&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;and &lt;a href=&#34;https://twitter.com/_anohito&#34; target=&#34;_blank&#34;&gt;Michitaka Hirose&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;downloads-1&#34;&gt;Downloads:&lt;/h4&gt;

&lt;p&gt;&lt;a href=&#39;https://jotaros.github.io/transcalibur/shigeyama-preprint.pdf&#39;&gt;Preprint Paper (PDF 7MB)&lt;/a&gt; -//-  &lt;a href=&#39;https://jotaros.github.io/transcalibur/transcalibur-movie.mp4&#39;&gt; Movie (MP3 20MB)&lt;/a&gt;  -//-    &lt;a href=&#39;https://jotaros.github.io/transcalibur/presskit.zip&#39;&gt; Press Kit (ZIP Archive 20MB)&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&#34;archive&#34;&gt;Archive:&lt;/h4&gt;

&lt;p&gt;Previously presented SIGGRAPH2018 E-tech Promotion can be seen &lt;a href=&#34;http://www.cyber.t.u-tokyo.ac.jp/~jotaro/transcalibur_web/&#34; target=&#34;_blank&#34;&gt;here.&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>知覚のベイズ統計的モデル:感覚強度曲線(1)</title>
      <link>https://jotaros.github.io/post/psycho/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0100</pubDate>
      
      <guid>https://jotaros.github.io/post/psycho/</guid>
      <description>

&lt;p&gt;ざっくりいうと&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;備忘録。&lt;/li&gt;
&lt;li&gt;ベルヌーイ分布をベイズ推論しているだけですが、感覚強度を測定する上で理解が深まりそうなのでまとめただけ&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;感覚を測定する&#34;&gt;感覚を測定する&lt;/h2&gt;

&lt;p&gt;かねてから認知心理学など、人間の知覚に関する分野では、知覚を測る試みがなされてきた。
近年はVRの目覚ましい発展により、多くの開発者・研究者が、VR空間で感覚を提示する上での様々な技術を創り出してきた。
高度なマルチモーダルインタフェースを作る上で、人間の感覚をある程度理解した上で設計できれば、様々なシチュエーションで提示する感覚をシステマチックにデザインできる。&lt;/p&gt;

&lt;p&gt;本稿では筆者が卒論で少しかじった、感覚強度の測定における統計的モデリングについて、備忘録も兼ねていくつかの記事に分けながら残しておく(内部向けにいろいろ残してはいたがなんかもったいない気がした)。
なお間違っている点など多々あるかと思われるので、クレームは@JotaroUTにて謹んでお受けいたします。&lt;/p&gt;

&lt;h3 id=&#34;感覚が生起することとは&#34;&gt;感覚が生起することとは&lt;/h3&gt;

&lt;p&gt;人間の感覚は、人間の感覚器官への感覚刺激の入力が脳内のニューラルネットワークで処理されることで、主観的な感覚へと帰着される。視覚や聴覚を司る脳の受容野は、(驚くべきことに)入力を物理的・機械的に処理するための規則正しいニューラルネットが分布していることがわかっており、特に低次の視覚受容野を模して作られたニューラルネットワークとしてCNN(Convolutional Neural Network)がよく知られている。&lt;/p&gt;

&lt;p&gt;意識レベルでの感覚の生起はさらに脳内での統合・認知や比較といった高レベルのプロセスを経ることによって行われるとされている。
[自主規制]したマウスの脳に[自主規制]して[自主規制]なりすれば、ニューロンの発火に伴う電気信号を直接観測することも可能だが、それでもなお高次の認知プロセスを物理レベルで測定するのは、fMRIなどのよっぽど高度な機材を使ったとしても定量的に評価することは難しい（しかし、すでに&lt;a href=&#34;https://www.biorxiv.org/content/10.1101/240317v2&#34; target=&#34;_blank&#34;&gt;こういう研究成果&lt;/a&gt;も出てきてはいるので、実は必ずしもとても難しいという時代ではないのかもしれない）。&lt;/p&gt;

&lt;h3 id=&#34;知覚測定と心理統計学&#34;&gt;知覚測定と心理統計学&lt;/h3&gt;

&lt;p&gt;よって、直接観測することの難しい感覚は一般的な実験室では、ある被験者が認知した結果を回答してもらい、その結果をまとめて分析・比較する手法が取られる。
実験の確度を増すために被験者を増やして多くのデータを取り、それらを正しく分析するために &lt;strong&gt;統計学&lt;/strong&gt; の知識が用いられる。特に心理学の分野において統計心理学として応用されている分野にあたる。
統計心理学のプロセスには統計処理のみならず、実験の方法やセットアップ、感覚刺激の提示方法や順序など、実験の &lt;strong&gt;統制(=control)&lt;/strong&gt; をするためのノウハウを組み込んでいる。
&lt;!-- これは認知の個人差などの雑音(noise)から、目的の情報(signal)を取り出すために必要であり、これがちゃんとできていないとどうにもならないことになる。 --&gt;&lt;/p&gt;

&lt;p&gt;感覚の強度を測定する実験として、19世紀頃ドイツの学者のウェーバー(Ernst Heinrich Weber)やフェヒナー(Gustav Fechner)によって、「ある基準の刺激に対して、提示した刺激が、より&amp;rdquo;大きい&amp;rdquo;と答えた確率を調べる」というものが初めて行われた。刺激量を大きくしたときの確率が50%のときの刺激量を「&lt;strong&gt;弁別閾&lt;/strong&gt;」と呼ぶ。また、一般的に刺激量を増やしていったときの回答確率は、なめらかなS字のカーブを描くことがわかっている。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://jotaros.github.io/img/weber-fechner.png&#34; alt=&#34;Weber-Fechner&#34; /&gt;
&lt;em&gt;-人間の知覚強度に関する &amp;ldquo;ウェーバー・フェヒナーの法則&amp;rdquo;は有名。比較刺激の弁別閾は標準刺激の量に対して対数的に増加する。&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;頻度主義的な感覚強度推定-2afc-ベルヌーイ分布&#34;&gt;頻度主義的な感覚強度推定：2AFC・ベルヌーイ分布&lt;/h2&gt;

&lt;h3 id=&#34;感覚刺激の統計的モデル&#34;&gt;感覚刺激の統計的モデル&lt;/h3&gt;

&lt;p&gt;人間の感覚強度の実験において、&lt;em&gt;ある一定の刺激量&lt;/em&gt; に対する平均回答確率を推定したいとする。
例えば、次のようなシンプルな実験を考える。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;基準となるおもり(=標準刺激)に対して、わずかに重いおもり(=比較刺激)を用意し、どちらかが重いかを回答してもらう。重い方の重りに対して回答のあった割合を求める。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;このような場合、感覚処理の過程で、&lt;em&gt;ある一定の確率で重い方を重いと回答する&lt;/em&gt; と仮定する。このとき提示刺激に対する回答$x=1$が得られる確率を$p$とすると、&lt;/p&gt;

&lt;p&gt;$$  x =
\begin{cases}
1,  &amp;amp; \text{with probability  $p$} \\&lt;br /&gt;
0, &amp;amp; \text{with probability  $1-p$}
\end{cases}$$&lt;/p&gt;

&lt;p&gt;このように、一定の確率で$X=1$が得られ、それ以外の場合は$X=0$となる確率分布はベルヌーイ分布として知られている。つまりこの実験において「人間の重さの感覚処理はベルヌーイ分布に従う」と仮定している。$X$を確率変数として記述すると、&lt;/p&gt;

&lt;p&gt;$$  {\rm Bern}(X | p) =
\begin{cases}
p,  &amp;amp; \text{for $X = 1$} \\&lt;br /&gt;
1-p, &amp;amp; \text{for $X = 0$}
\end{cases}$$&lt;/p&gt;

&lt;p&gt;つまり&lt;/p&gt;

&lt;p&gt;$$
{\rm Bern}(X|p) = p^X (1-p)^{1-X}
$$&lt;/p&gt;

&lt;p&gt;この確率$p$こそ、「刺激が$p$の確率で識別できるに足りる感覚の強度」として捉えることができる。先のウェーバー・フェヒナーの実験は、この確率をもとに人間の感覚強度を測定・推定している。&lt;/p&gt;

&lt;h3 id=&#34;ベルヌーイ分布の-頻度主義的-最尤推定&#34;&gt;ベルヌーイ分布の(頻度主義的)最尤推定&lt;/h3&gt;

&lt;p&gt;頻度主義的に考えれば、得られたデータから、先に仮定したベルヌーイ分布の真のパラメータ$p$を推定するためには尤度最大化のアプローチで最尤推定を用いる(ベルヌーイ分布のようなシンプルなモデルの場合は、直感的には得られた実際のデータの回答確率$\hat{p}$が最尤推定だろうと感じられる)。実際に確かめてみる。&lt;/p&gt;

&lt;p&gt;得られたデータ列$\mathcal{D}=\{ X_1, X_2, &amp;hellip; ,X_n \}$について尤度関数を$L(p|\mathcal{D})$とすれば、&lt;/p&gt;

&lt;p&gt;$$
\begin{eqnarray}
L(p|\mathcal{D}) &amp;amp;  = &amp;amp; \prod_{i=1}^n {\rm Bern}(X_i|p) \\&lt;br /&gt;
\frac{\partial}{\partial \hat p} &amp;amp; = &amp;amp; \sum_{i=1}^n\ln{\rm Bern}(X_i|\hat p) \\&lt;br /&gt;
&amp;amp; = &amp;amp; \sum_{i=1}^n \frac{X_i}{\hat p} - \frac{(1-X_i)}{1- \hat p} = 0 \\&lt;br /&gt;
\iff \hat p &amp;amp; = &amp;amp; \frac{1}{n}\sum_{i=1}^nX_i
\end{eqnarray}$$&lt;/p&gt;

&lt;p&gt;確かに、真のパラメータ$p$に対する最尤推定は提示刺激に対する回答確率をもって最尤推定できる。&lt;/p&gt;

&lt;h2 id=&#34;ベイズ主義的な感覚強度推定-2afc-ベルヌーイ分布&#34;&gt;ベイズ主義的な感覚強度推定：2AFC・ベルヌーイ分布&lt;/h2&gt;

&lt;p&gt;心理統計学の分野では、先程のような実験のように不確実性を含んだ問題をとく上で、頻度主義な統計手法だけではなく、ベイズ的な統計を扱うこともある。講談社の”ベイズ推論による機械学習入門”では、ベイズ統計を問題に応用する上での利点として&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;さまざまな問題が一貫性をもって解ける&lt;/li&gt;
&lt;li&gt;対象の不確実性を定量的に取り扱うことができる&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;などといった点をあげている。知覚モデルを構築・推論するといったプロセスがベイズ的アプローチで記述できるという点だけはなく、認知心理学における事前知識をベイズ推論に組み込むこともできるといった利点もある。特に、感覚強度測定のように多くのデータ点を取らなければならない実験系では、測定した結果がどの程度信頼できるものなのかを定量的に知ることも重要となってくる。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;確率推論に基づく解析手法では「袋$a$か$b$か」といった２択の結論をだすのではなく、それぞれの原因がどれほどの確かさであったのかを定量的に表すことができます。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;改めて強調しておくと、感覚強度のように実世界で様々な影響を排除できない不確実なパラメータを推定する上で、その結果がどれほど信頼できるものかを定量的に評価することは、主観的認知プロセスを主とするシステムをデザインする上で重要なファクターと言える。たとえば未来のVRChat等の多くのユーザが関わるひとつの広大なVRシステムに、実は人間の知覚モデルを取り入れたフィードバックループが構築されていて、パラメータの信頼区間などが考慮されている、強いて言えば処理系がモデルと処理結果の因果関係を記述的に推論しているシステムがあるとすれば、なんか素敵ではないだろうかと思う。&lt;/p&gt;

&lt;h3 id=&#34;ベイズの公式とパラメータ推論&#34;&gt;ベイズの公式とパラメータ推論&lt;/h3&gt;

&lt;p&gt;言わずとしたベイズの公式は、データ$\mathcal{D}$とパラメータ$\theta$を確率変数とした場合、&lt;/p&gt;

&lt;p&gt;$$
\begin{eqnarray}
p(\theta|\mathcal{D}) &amp;amp;=&amp;amp; \frac{p(\theta)p(\mathcal{D}|\theta)}{p(D)} \\&lt;br /&gt;
&amp;amp;\propto&amp;amp; p(\theta)p(\mathcal{D}|\theta)
\end{eqnarray}
$$&lt;/p&gt;

&lt;p&gt;として表される。事後分布(posterior)$p(\theta | D)$は尤度(likelihood)$p(\theta)$と事前分布(prior)$p(D|\theta)$を掛け合わせて正規化した結果で表されるというものだ。ここでハイパーパラメータとなるのは_初期の事前分布_である。極端な話よくわからなければとりあえず一様分布にしたくなるが、今回の知覚実験のように、ベルヌーイ分布という（強めの）仮定をしている場合は、事前分布の選択肢は絞られる。&lt;/p&gt;

&lt;h3 id=&#34;ベルヌーイ分布に対する共役事前分布-conjugate-prior&#34;&gt;ベルヌーイ分布に対する共役事前分布 (Conjugate Prior)&lt;/h3&gt;

&lt;p&gt;事後分布の関数形が、事前分布に尤度を掛け合わせた場合と同じ関数形となる場合、そのように選んだ事前分布を(自然な)共役事前分布(Conjugate prior)と呼ぶ。
ベルヌーイ分布の場合は&lt;strong&gt;ベータ分布&lt;/strong&gt;と呼ばれる0から1までの実数を返す分布がその共役事前分布にあたる。
$$\begin{eqnarray}
\beta(\theta|a, b) &amp;amp;=&amp;amp; \frac{\theta^{a-1}(1-\theta)^{b-1}}{B(a,b)} \\&lt;br /&gt;
B(a,b) &amp;amp;=&amp;amp; \int_0^1 t^{a-1} (1-t)^{b-1}dt \, \text{(Beta function)}
\end{eqnarray}$$&lt;/p&gt;

&lt;h3 id=&#34;事後分布の計算&#34;&gt;事後分布の計算&lt;/h3&gt;

&lt;p&gt;知覚実験によって得られたデータ$\mathcal{D}$によって計算される尤度関数は、各試行が独立の場合、&lt;/p&gt;

&lt;p&gt;$$
p(\mathcal{D}|\theta) = \prod_{i=1}^np(X_i|\theta)
$$
となる。
共役事前分布の計算の中でも特にわかりやすいのでネットで検索すると&lt;a href=&#34;https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/other-readings/chapter9.pdf&#34; target=&#34;_blank&#34;&gt;いろいろ出てくる&lt;/a&gt;が、実際にベルヌーイ分布と掛け合わせる場合は対数を用いて、&lt;/p&gt;

&lt;p&gt;$$\begin{eqnarray}
\ln p(\theta|\mathcal{D}) &amp;amp; \propto &amp;amp; \left(\sum_{i=1}^N x_i + a - 1 \right)\ln \theta + \left(N - \sum_{i=1}^N + b - 1\right) \ln(1-\theta) + {\it const.} \\&lt;br /&gt;
&amp;amp; = &amp;amp; (\hat a -1) \ln \theta + (\hat b -1) \ln(1-\theta) + {\it const.}\\&lt;br /&gt;
\end{eqnarray}$$&lt;/p&gt;

&lt;p&gt;となり、&lt;/p&gt;

&lt;p&gt;$$\begin{eqnarray}
\hat a &amp;amp;=&amp;amp; \sum_{i=1}^N x_i + a \\&lt;br /&gt;
\hat b &amp;amp;=&amp;amp; N - \sum_{i=1}^N + b
\end{eqnarray}$$&lt;/p&gt;

&lt;p&gt;とすれば、事後分布の関数形はたしかに新しいβ関数 $p(\theta|\mathcal{D})=beta(\theta|\hat a , \hat b)$となっている。&lt;/p&gt;

&lt;p&gt;逐次推論をするときに繰り返し計算となる場合にこのような単純な変数の更新で済むとプログラムに書き起こすときなどにかなり簡単になって嬉しい一方、共役事前分布を使わないと繰り返し数値計算をするはめになったりしてめんどくさいことになる。しかしながら共役事前分布が解析的に見つかることのほうが珍しいらしいので、統計的数値計算を用いた方法もいろいろと存在する。&lt;/p&gt;

&lt;h3 id=&#34;予測分布の計算&#34;&gt;予測分布の計算&lt;/h3&gt;

&lt;p&gt;事後分布の計算では観測したデータに基づいてパラメータを確率的に推論したが、未知のデータ$X_*$に対する尤度を求めた事後分布で重み付け計算(=周辺化)した予測分布を導出できる。つまり確率的に推論した結果をもって、ベルヌーイ分布の予測分布を計算する。&lt;/p&gt;

&lt;p&gt;$$
p(X_* | X) = \int {\rm Bern}(X_* | \theta ) \beta ( \theta, | \hat a, \hat b, X) d\theta
$$&lt;/p&gt;

&lt;p&gt;周辺化は&lt;a href=&#34;https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/other-readings/chapter9.pdf&#34; target=&#34;_blank&#34;&gt;この資料の式9.31あたり&lt;/a&gt;を参考にすると、&lt;/p&gt;

&lt;p&gt;$$
p(X_*| X) = {\rm Bern} \left( X_* | \frac{\hat a}{\hat a + \hat b} \right)
$$&lt;/p&gt;

&lt;p&gt;という計算に帰着する。このベルヌーイ分布が、事前分布を$\beta(\theta|a, b)$として仮定し学習させた場合、新たに来るデータ＝確率変数に対する予測分布となるはず。&lt;/p&gt;

&lt;p&gt;以上のように、同一刺激下でのベイズ知覚モデルが設計できた。&lt;/p&gt;

&lt;h2 id=&#34;実験&#34;&gt;実験&lt;/h2&gt;

&lt;p&gt;実際に被験者実験を想定したデータ収集の状況下で感覚強度のベイズ推論をしてみる。&lt;/p&gt;

&lt;h3 id=&#34;実験設定&#34;&gt;実験設定&lt;/h3&gt;

&lt;p&gt;被験者は10名。標準刺激に対して対象となる比較刺激を一人あたり25回提示する。
(実際の実験室では)標準刺激と比較刺激はそれぞれどちらかを先に提示し、１施行あたりランダムな順番で提示するものとする。
10名には被験者内誤差があり、$\theta = 0$の周辺で標準誤差$\sigma = 0$で分布しているとし、スクリプトでi.i.dのデータを生成する。&lt;/p&gt;

&lt;h3 id=&#34;pythonによるコード&#34;&gt;Pythonによるコード&lt;/h3&gt;

&lt;pre&gt;
&lt;code class=&#34;language-python&#34;&gt;# coding:utf-8
import numpy as np
import seaborn
import matplotlib.pyplot as plt
from numpy.random import *
from scipy.stats import bernoulli, beta

%matplotlib inline

# experiment parameters
num_subjects = 10
num_trials = 25
mean_prob = 0.75
var_subs  = 0.05

# virtual experiment
N = num_subjects * num_trials
sum_aye=0
params = []
for _ in range(10):
    theta = min(1, normal(mean_prob, var_subs))
    params.append(theta)
    sum_aye += sum(filter(lambda n:n%2==1, bernoulli.rvs(theta, size=num_trials)))

# inference
init_params = [2.5,2.5]
post_params = [sum_aye + init_params[0], N-sum_aye+init_params[1]] # = (a hat, b hat)
x = np.linspace(0, 1, 1000)
prior = beta(init_params[0], init_params[1])
posterior = beta(post_params[0], post_params[1])
y = posterior.pdf(x)
y_prior = prior.pdf(x)

# plot
f, (ax1, ax2) = plt.subplots(1, 2, sharey = True)
f.set_figheight(3)
f.set_figwidth(10)

ax1.set_title(&#39;prior distribution&#39;)
ax1.grid(color=&#39;gray&#39;, linestyle=&#39;-&#39;, linewidth=1, alpha = 0.2)
ax1.plot(x, y_prior)
ax1.fill(x, y_prior, &#39;blue&#39;, alpha=0.1)
ax1.vlines(params, 0, 1, color=&#39;k&#39;, linewidth=0.5)
ax1.vlines(0.75, 0, 1, color=&#39;r&#39;, linewidth=2.0)


ax2.set_title(&#39;posterior distribution&#39;)
ax2.grid(color=&#39;gray&#39;, linestyle=&#39;-&#39;, linewidth=1, alpha = 0.2)

ax2.plot(x, y)
ax2.fill(x, y, &#39;blue&#39;, alpha=0.1)
ax2.vlines(params, 0, 1, color=&#39;k&#39;, linewidth=0.5)
ax2.vlines(0.75, 0, 1, color=&#39;r&#39;, linewidth=2.0)

f.savefig(&#39;plot.svg&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;事後分布の結果&#34;&gt;事後分布の結果&lt;/h3&gt;

&lt;p&gt;グラフに中心パラメータ$(\theta=0.75)$と、各被験者に割り当てられたパラメータをそれぞれ赤線と黒線で示した。事後分布は施行を経るごとに一つの値に収束していく事がわかる。試行回数を増やせば、事後分布はさらに収束することもわかる。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://jotaros.github.io/img/plot.svg&#34; alt=&#34;Plot&#34; /&gt;
&lt;em&gt;N=10、各25回施行&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://jotaros.github.io/img/100times.svg&#34; alt=&#34;Plot&#34; /&gt;
&lt;em&gt;N=10、各100回施行&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&#34;予測分布の結果&#34;&gt;予測分布の結果&lt;/h3&gt;

&lt;p&gt;予測分布は$(\theta=0.75)$に近い値となった。被験者間誤差に寄る多少のばらつきがあるが、パラメータは被験者間平均に近い値を予測できていることがわかる。&lt;/p&gt;

&lt;pre&gt;
&lt;code class=&#34;language-python&#34;&gt;# predictive distribution
post_theta = post_params[0]  / (post_params[0]+post_params[1])
post_theta

# probability for X=1 on predicted distribution
# result(N=25, sample:25)   0.73529411764705888
# result(N=25, sample:100)  0.74079601990049748
# result(N=25, sample:1000) 0.75817091454272867
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;まとめ&#34;&gt;まとめ&lt;/h2&gt;

&lt;p&gt;感覚強度の仮想実験にベイズ統計を用いたパラメータ推定を考えてみた。機械学習や人工知能の問題がベイズ推論で記述されているのと同じく、このような心理統計学の分野での問題もベイズ推論が用いられるというのは、知能や認知とベイズ推論に何かしら根本的なつながりを感じさせるものだと思う。
今回は限定的に、実験において一定の刺激のみを提示したが、実際に弁別閾を測定したり、感覚強度曲線をプロットするときは、曲線パラメータの推定が必要になる。
そのような場合は今回のように直接的にモデルをたてることが難しくなる場合がある。また、事後確のの計算や統計モデルの妥当性検討・逸脱度検定なども必要になるために、MCMCやBootstrap法といったテクニックも必要になってくる。次回の記事ではそのあたりに触れて、実際に感覚強度曲線のベイズ推論について書く。&lt;/p&gt;

&lt;h3 id=&#34;参考文献&#34;&gt;参考文献&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Wichmann, Felix A., and N. Jeremy Hill. &amp;ldquo;The psychometric function: I. Fitting, sampling, and goodness of fit.&amp;rdquo; Perception &amp;amp; psychophysics 63.8 (2001): 1293-1313.&lt;/li&gt;
&lt;li&gt;Stat 260/CS 294 Bayesian Modeling and Inference, UC Berkley &lt;a href=&#34;https://www.biorxiv.org/content/10.1101/240317v2&#34; target=&#34;_blank&#34;&gt;https://www.biorxiv.org/content/10.1101/240317v2&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>DualPanto: A Haptic Device that Enables Blind Users to Continuously Interact with Virtual Worlds</title>
      <link>https://jotaros.github.io/project/dualpanto/</link>
      <pubDate>Mon, 20 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>https://jotaros.github.io/project/dualpanto/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;https://jotaros.github.io/img/dualpanto.png&#34; alt=&#34;Pantograph&#34; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;We present a new haptic device that enables blind users to continuously track the absolute position of moving objects in spatial virtual environments, as is the case in sports or shooter games. Users interact with DualPanto by operating the me handle with one hand and by holding on to the it handle with the other hand.&lt;/p&gt;

&lt;p&gt;Each handle is connected to a pantograph haptic input/output device. The key feature is that the two handles are spatially registered with respect to each other. When guiding their avatar through a virtual world using the me handle, spatial registration enables users to track moving objects by having the device guide the output hand.&lt;/p&gt;

&lt;p&gt;This allows blind players of a 1-on-1 soccer game to race for the ball or evade an opponent; it allows blind players of a shooter game to aim at an opponent and dodge shots. In our user study, blind participants reported very high enjoyment when using the device to play (6.&lt;sup&gt;5&lt;/sup&gt;&amp;frasl;&lt;sub&gt;7&lt;/sub&gt;).&lt;/p&gt;

&lt;p&gt;For more details visit &lt;a href=&#34;https://hpi.de/baudisch/projects/dualpanto.html&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;publication&#34;&gt;Publication&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Oliver Schneider, &lt;u&gt;&lt;strong&gt;Jotaro Shigeyama&lt;/strong&gt;&lt;/u&gt;, Robert Kovacs, Thijs Jan Roumen, Sebastian Marwecki, Nico Boeckhoff, Patrick Baudisch, DualPanto: A Haptic Device that Enables Blind Users to Continuously Interact with Virtual Worlds,
In Proceedings of UIST&amp;rsquo;18, October, &lt;strong&gt;&lt;em&gt;TO APPEAR&lt;/em&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>引っ越しするので家具を売ります。</title>
      <link>https://jotaros.github.io/post/hikkoshi/</link>
      <pubDate>Sat, 20 Jan 2018 00:00:00 +0900</pubDate>
      
      <guid>https://jotaros.github.io/post/hikkoshi/</guid>
      <description>

&lt;p&gt;ざっくりいうと：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ドイツの大学院に編入します。&lt;/li&gt;
&lt;li&gt;東京の家は引き払うので、家具とかを売り出し致します。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;家具-売ります&#34;&gt;家具、売ります。&lt;/h2&gt;

&lt;p&gt;ドイツの大学院にうつることになりました。&lt;/p&gt;

&lt;p&gt;以下の家具、いらないので売ります。（輸送どうしようかとかはあんまり考えてない・・・）&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;LGの23?型モニタ (from 12000 yen)

&lt;ul&gt;
&lt;li&gt;ゲームする用に購入したやつ&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;オフィスチェア（Ergohuman ENJOY) (from 31000 yen)&lt;/li&gt;
&lt;li&gt;TV(30型ぐらい？ Sharp レグザ)&lt;/li&gt;
&lt;li&gt;本棚 (無印のスタックシェルフと、4段ぐらいの白い本棚)&lt;/li&gt;
&lt;li&gt;自転車（GIANT Escape r3) (from 13000 yen)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;（後日追加予定。）&lt;/p&gt;

&lt;p&gt;Update :2018年1月20日15時32分
- 家具人気だったので「譲る」→「売る」に変更。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hugoでアカデミックなポートフォリオサイト兼ブログをつくってみた</title>
      <link>https://jotaros.github.io/post/hugo-theme/</link>
      <pubDate>Wed, 20 Dec 2017 00:00:00 +0900</pubDate>
      
      <guid>https://jotaros.github.io/post/hugo-theme/</guid>
      <description>

&lt;p&gt;修士以降の学生で研究に取り組んでいると、自分のプロジェクトやpublicationをまとめたシンプルなウェブページが欲しくなってくる。Wordpressやstrikinglyみたいなサービスが出ているが、オリジナルのWebページを作りたいときには制約が多かったり、機能が多すぎたりすることがある。&lt;/p&gt;

&lt;p&gt;HugoはGo言語で記述された、 &lt;strong&gt;スゴイハヤイ静的ウェブジェネレータである。&lt;/strong&gt; 記事やプロジェクトなどを &lt;strong&gt;Markdown記法&lt;/strong&gt; で記述することで投稿したり更新したりできるため、普段mdでノート取ってる方やQiitaの諸兄・諸姉には非常に使い勝手のいいWebジェネレータであると思う。&lt;/p&gt;

&lt;p&gt;今回は &lt;code&gt;Academic&lt;/code&gt; テーマをベースにオリジナルページを作成してみた。&lt;/p&gt;

&lt;p&gt;ウェブサイトはこちら : &lt;a href=&#34;https://jotaros.github.io&#34; target=&#34;_blank&#34;&gt;web&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;使い方-初めての人へ&#34;&gt;使い方（初めての人へ）&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://gohugo.io/getting-started/quick-start/&#34; target=&#34;_blank&#34;&gt;https://gohugo.io/getting-started/quick-start/&lt;/a&gt;
チュートリアルはここから&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;brewでインストール&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;brew install hugo
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;プロジェクトを生成&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;hugo new site yourSiteName
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;テーマを導入&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cd yourSiteName
git init
git submodule add https://github.com/budparr/gohugo-theme-ananke.git themes/ananke

echo &#39;theme = &amp;quot;ananke&amp;quot;&#39; &amp;gt;&amp;gt; config.toml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ここでは、yourSiteNameプロジェクト内のthemeフォルダに、submoduleとしてテーマプロジェクトを追加している。
テーマプロジェクト内は基本的には生成したプロジェクトと同様の構造となっており、exampleもあるので改造しやすい。
プロジェクトを追加したときに生成されるconfig.tomlを書き換えてテーマをアクティブにする必要がある。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;記事を投稿&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;hugo new posts/my-first-post.md
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;config.tomlに以下の文言を追加して、&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;baseURL = &amp;quot;https://example.org/&amp;quot;
languageCode = &amp;quot;en-us&amp;quot;
title = &amp;quot;My New Hugo Site&amp;quot;
theme = &amp;quot;ananke&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;サイトを生成（ビルド）&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;hugo server -D
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&#34;localhost:1337&#34; target=&#34;_blank&#34;&gt;localhost:1337&lt;/a&gt;にアクセスすると、生成されたウェブサイトが見れる。たったこれだけでWEBサイトが完成してしまう。&lt;/p&gt;

&lt;h2 id=&#34;academicテーマの改造&#34;&gt;Academicテーマの改造&lt;/h2&gt;

&lt;p&gt;インストールは、git submoduleで&lt;a href=&#34;https://themes.gohugo.io/theme/academic/&#34; target=&#34;_blank&#34;&gt;Academicテーマ&lt;/a&gt;を&lt;code&gt;themes&lt;/code&gt;フォルダに追加すればよい。exampleSiteの投稿（content）ツリーはこのようになっている。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;├── exampleSite
│   ├── config.toml
│   ├── content
│   │   ├── home
│   │   │   ├── about.md
│   │   │   ├── contact.md
│   │   │   ├── hero.md
│   │   │   ├── posts.md
│   │   │   ├── projects.md
│   │   │   ├── publications.md
│   │   │   ├── publications_selected.md
│   │   │   ├── tags.md
│   │   │   ├── talks.md
│   │   │   └── teaching.md
│   │   ├── post
│   │   │   ├── _index.md
│   │   │   ├── faq.md
│   │   │   ├── getting-started.md
│   │   │   ├── managing-content.md
│   │   │   ├── migrate-from-jekyll.md
│   │   │   ├── widgets.md
│   │   │   └── writing-markdown-latex.md
│   │   ├── project
│   │   │   ├── deep-learning.md
│   │   │   └── example-external-project.md
│   │   ├── publication
│   │   │   ├── _index.md
│   │   │   ├── clothing-search.md
│   │   │   └── person-re-identification.md
│   │   ├── tags
│   │   │   └── academic
│   │   │       └── _index.md
│   │   └── talk
│   │       ├── _index.md
│   │       └── example-talk.md
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;トップページに現れるプロフィール画像や文言などもすべてmd記法で記述することが可能。&lt;/p&gt;

&lt;h3 id=&#34;cssでカスタマイズ&#34;&gt;cssでカスタマイズ&lt;/h3&gt;

&lt;p&gt;元のデザインも十分に良いのだが、自分風にカスタマイズしたい場合は、&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;(theme側ではなく)自分のプロジェクトの &lt;code&gt;static&lt;/code&gt; フォルダに、 &lt;code&gt;custom.css&lt;/code&gt; という名前でcssファイルを作成&lt;/li&gt;
&lt;li&gt;主に &lt;code&gt;partials/widjets&lt;/code&gt; などの要素・クラスを見ながら、custom.cssを書きかえ&lt;/li&gt;
&lt;li&gt;必要に応じてテンプレートを書き換え（上級者向き）、削除などをする（talkしたことなければ消したりなど）。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;このあたりの細かい作業が（Hugo初めてだったので）Hard-Codedになってしまったのは大いに反省なのだが、上に書いてあるフォルダ内のデータをいじれさえすれば、Academicのpostパターンを崩すことなくオリジナルのポートフォリオサイトが作成できる。&lt;/p&gt;

&lt;p&gt;作成したウェブサイトがこちら：&lt;a href=&#34;https://jotaros.github.io&#34; target=&#34;_blank&#34;&gt;web&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;まとめ&#34;&gt;まとめ&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;HugoはスゴイハヤイstaticWebジェネレータ&lt;/li&gt;
&lt;li&gt;mdで書けるため、普段qiitaにおられる諸兄・諸姉におすすめ&lt;/li&gt;
&lt;li&gt;Academicは研究者・学生向けのテンプレートとして有能&lt;/li&gt;
&lt;li&gt;他のテンプレートエンジンと同様、partialsをいじってcssを書き換えるなどして簡易にオリジナルデザインに変更できる。&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>ドイツのHCI系トップ研究室に行ってインターンしてきた話</title>
      <link>https://jotaros.github.io/post/kenkyu-ryugaku/</link>
      <pubDate>Thu, 07 Dec 2017 00:00:00 +0900</pubDate>
      
      <guid>https://jotaros.github.io/post/kenkyu-ryugaku/</guid>
      <description>

&lt;p&gt;ざっくりいうと：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;自己紹介：&lt;a href=&#34;https://www.cyber.t.u-tokyo.ac.jp&#34; target=&#34;_blank&#34;&gt;あの人の研究室&lt;/a&gt;の修士１年生&lt;/li&gt;
&lt;li&gt;いつ行ったか：2017/7 - 9&lt;/li&gt;
&lt;li&gt;どこへ行ったか：ドイツ・ポツダム大学 Hasso Plattner Insitute, HCI Group&lt;/li&gt;
&lt;li&gt;何をやったか：HCIに関する研究(あまりまだ詳しく言えないですが触覚系です)&lt;/li&gt;
&lt;li&gt;どうやって行ったか：&lt;a href=&#34;https://twitter.com/drinami&#34; target=&#34;_blank&#34;&gt;@drinami&lt;/a&gt;先生からの紹介。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;はじめての海外留学&#34;&gt;はじめての海外留学&lt;/h2&gt;

&lt;p&gt;この度、ドイツ・ポツダムの&lt;a href=&#34;https://www.hpi.de&#34; target=&#34;_blank&#34;&gt;Hasso Plattner Institute&lt;/a&gt;にある&lt;a href=&#34;https://hpi.de/baudisch/home.html&#34; target=&#34;_blank&#34;&gt;HCIラボ&lt;/a&gt;にお邪魔して、実質1ヶ月とちょいでCHIのフルペーパーをSubmitした研究インターンについて書きたいと思います。
どのみちこの体験を何かしら文章にして残したいなとは思っていたのですが、おそらくすんごい人たちがたくさん集まるであろうAdvent Calendarへ誘っていただいた&lt;a href=&#34;https://twitter.com/keihigu&#34; target=&#34;_blank&#34;&gt;keihigu&lt;/a&gt;さんには、知り合って間もないにも関わらずお気遣いいただき感謝致します。稚拙な記事かと思いますが、これから研究インターンに行こうと思っている方に少しでも参考になれば幸いです。思いついたことを思いついた順に書いていくので、支離滅裂な記事になるかもしれませんが、がんばるぞいなんでご容赦願います。&lt;/p&gt;

&lt;h3 id=&#34;かるく自己紹介and何故行こうと思ったか&#34;&gt;かるく自己紹介and何故行こうと思ったか&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://jotaros.github.io&#34; target=&#34;_blank&#34;&gt;Jotaro&lt;/a&gt; といいます。出身は山口県で、高専の機械電気科から東大の機械情報へ進学して、現在 &lt;a href=&#34;https://www.cyber.t.u-tokyo.ac.jp&#34; target=&#34;_blank&#34;&gt;あの人の研究室&lt;/a&gt; で学際情報学府のM1をやっています。&lt;/p&gt;

&lt;p&gt;そもそもこの研究留学に行くことになったのはほんの一瞬の出来事がきっかけで、3月頃に&lt;a href=&#34;https://twitter.com/drinami&#34; target=&#34;_blank&#34;&gt;@drinami&lt;/a&gt;先生に突如四川料理屋さんに呼び出されて、お酒飲んでいい感じになった勢いで「HPI行かない？」と言われたことでした(同じ席に &lt;a href=&#34;https://twitter.com/keihigu&#34; target=&#34;_blank&#34;&gt;keihigu&lt;/a&gt;さんもいましたね)。ちょうど卒論が終わったにも関わらず、自分の研究の方針やラボでの立ち位置や人間関係にかなり悩まされていた時期だったこと、高専時代にもドイツ人の教官のもとで2年間過ごしていたことからもともとドイツという国に興味が深かったこと、高々2-3週間しか海外に滞在したことのなかった自分にとっては貴重な経験になるだろうということが重なり、いいチャンスだと思ったのですぐに行こうと決意しました。大規模な研究室が多いイメージのアメリカではなく、あえてドイツへ行こうと思ったのもそういった理由があってこそです。&lt;/p&gt;

&lt;h3 id=&#34;日程&#34;&gt;日程&lt;/h3&gt;

&lt;p&gt;夏のとても良い時期に行くことが出来ました。7月下旬から10月頭まで、ビザ無しでインターンができる滞在期間のギリギリまでいました。おかげで帰りの出国審査の際に「お前ドイツに住んでんの？」と言われ、電卓で日程をめちゃ計算されて焦った覚えがあります。&lt;strong&gt;事前にビザ無しでインターンが可能か、インターン先や大使館に事前に相談しておくことをおすすめします。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;ドイツの夏は一瞬にして終わるため、7-8月は特にドイツ人は外で遊び、日光を浴びたりお祭り騒ぎをやったりしている印象です。街中がいい感じに賑わっていて、いい感じでした。&lt;/p&gt;

&lt;h3 id=&#34;ドイツという国-ベルリンという場所について&#34;&gt;ドイツという国・ベルリンという場所について&lt;/h3&gt;

&lt;p&gt;上にも書きましたが、最初にドイツという国に興味が湧いたのは高専の頃でした。
世界史担当の教官の先生には、中世Lübeck地域に関する歴史を専門にしていたことから、不要になったドイツ語の本をよく譲っていただいたりしていました。
また卒業論文の教官がKleve出身のドイツ人理論物理学者で、高専の卒研の一年をドイツ語・英語で過ごすということもあったため、なにかしら縁のある国の一つでした。
HPIの研究はよく知っていましたが、まさかPotsdamにあるとは・・・という感じだったので、ますます行ってみたくなった、という感じです。&lt;/p&gt;

&lt;p&gt;そんなこんなでかねてよりドイツという国そのものにも興味がありました。
中世・神聖ローマ帝国時代から現代までキリスト教と切っても切れない関係にあったり、数多くの内紛や周辺諸国との戦争で何度もズタボロになったり、また帝政・共和政・ファシズム・そして冷戦時代の東西分裂からの再統一、などというバリエーションに富んだ歴史を歩みつつも、「統一ヨーロッパ・統一ドイツ」をテーマに掲げて今もなお進み続ける姿には、割と魅力を感じていました。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://jotaros.github.io/img/wall.jpg&#34; alt=&#34;Berlin wall&#34; /&gt;
&lt;em&gt;Mauerpark (Park of Wall), Berlin&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;ベルリンの町自体も19世紀あたりのプロイセン王国時代以降、ドイツ国(Deutsches Reich)の首都として機能していた一方、ファシズムや冷戦などの抑圧の時代を乗り越えてきました。その過程で多くの国籍や背景を持った人たちがごった返し、特に再統一後の廃墟をアーティストが占拠するなど、多くの経過（&lt;a href=&#34;https://lutemedia.com/post/from-berlin-report&#34; target=&#34;_blank&#34;&gt;この記事&lt;/a&gt;がよく書かれています)を辿った結果、現在よく知られている活気のある楽しい感じの街に発展していきました。自分で言うのも何ですが若いうちにこういう街を訪れる事ができて嬉しかったです。&lt;/p&gt;

&lt;p&gt;さて前置きが長くなりましたが、そろそろ研究インターン体験談を・・・m(_ _)m&lt;/p&gt;

&lt;h2 id=&#34;hpiとは&#34;&gt;HPIとは&lt;/h2&gt;

&lt;h3 id=&#34;一応は大学の学部-研究科&#34;&gt;一応は大学の学部＋研究科&lt;/h3&gt;

&lt;p&gt;HPI、もとい&lt;a href=&#34;https://www.hpi.de&#34; target=&#34;_blank&#34;&gt;Hasso Plattner Institute&lt;/a&gt;は、いわゆる指定管理者制度( &lt;em&gt;Öffentlich-private Partnerschaft&lt;/em&gt; )に基づいて公設民営化された、ポツダム大学の情報システム工学科という &lt;del&gt;ややこしい&lt;/del&gt; 立ち位置です。ヨーロッパ最大のソフトウェア企業であるSAPの共同創業者の一人：Hasso Plattner氏が立ち上げたもので、20年間で述べ2億ユーロを支出しています。サンキューハッソ。スタンフォード大学と共同でDesign Thinkingラボも有しており、静かな場所にある学校ですが色々と活発な研究が行われているそうです。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://jotaros.github.io/img/hpi-campus.jpg&#34; alt=&#34;HPI Hauptgebaeude&#34; /&gt;
&lt;em&gt;Hauptgebäude (Main Building), HPI, Potsdam&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&#34;hci研究室&#34;&gt;HCI研究室&lt;/h3&gt;

&lt;p&gt;HPIには情報系の研究室がいくつもありますが、そのうちの一つである&lt;a href=&#34;https://hpi.de/baudisch/home.html&#34; target=&#34;_blank&#34;&gt;HCIラボ&lt;/a&gt;は、HCI界隈では &lt;strong&gt;パト様&lt;/strong&gt; として知られる&lt;a href=&#34;http://patrickbaudisch.com/&#34; target=&#34;_blank&#34;&gt;Patrick Baudisch&lt;/a&gt; 先生の研究室です。&lt;/p&gt;

&lt;p&gt;毎年安定してCHI、UISTのBest Paperを取ったりしつつも、VRやハプティクス、メタマテリアルやファブリケーション技術など、固まった分野にとらわれず、数十年後の未来を見据えたあらゆるテーマに取り組んでいる研究室です。Ph.D Candidateの方が7人ぐらいいる、小さくても大きな研究室だなと言うイメージでした。&lt;/p&gt;

&lt;h2 id=&#34;研究インターン-始まる&#34;&gt;研究インターン、始まる&lt;/h2&gt;

&lt;p&gt;このラボの研究インターンは紹介を通して受けて申し込みましたが、さらに面接と教員からの推薦状が2通必要でした。&lt;/p&gt;

&lt;p&gt;基本的にラボの公用語は英語ですが、修士や学部生とのやり取りや昼飯時などは頻繁にドイツ語が飛び交います。私は海外経験はありませんでしたが、面接から日々の生活まで長年の洋ゲーやネトゲ経験で積み上げた英語力で乗り切りました。 &lt;strong&gt;当然ながら日本のラボで議論するよりも激しかったです。白熱した研究の議論に耐えうる強靭な語学力が要求されました。&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&#34;スケジュール感&#34;&gt;スケジュール感&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;3-4月：インターン申し込み・面接・インターンOK通知&lt;/li&gt;
&lt;li&gt;4-6月：研究なにやるかの打ち合わせ・方向性の検討&lt;/li&gt;
&lt;li&gt;6月上旬：学会でドイツに行く機会があったので一回ラボを見学&lt;/li&gt;
&lt;li&gt;6月下旬まで：宿(Airbnb)の手配・手続き&lt;/li&gt;
&lt;li&gt;7月中旬から下旬：ラボin　研究テーマの洗い出しと方向性の決定&lt;/li&gt;
&lt;li&gt;8月一杯：鬼実装。ひたすら鬼実装。&lt;/li&gt;
&lt;li&gt;9月中旬(paper deadline)まで：実験・論文執筆・submit&lt;/li&gt;
&lt;li&gt;9月下旬まで：vacationとお片付け。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Deadlineまで、一気に走り抜けていくような日々でした。研究の内容はPeer review中ということもあり詳しく書けませんが、日本で卒論やった時よりも「研究した感」がすごかったです。ビデオのとり方や図の書き方といった細かなテクニックから、論文執筆までの流れなど、短期間で多くを学ぶことが出来ました。&lt;/p&gt;

&lt;p&gt;やった内容に関して、まだ詳しく書けないのが残念です。「結局何やったんや！」って怒る人もいるかもしれませんが、もちょい待ってていただければ嬉しいです。
ラボの生活メインでどんな感じだったか、どうやったらテンポよくやっていけたかをメインに書きます。&lt;/p&gt;

&lt;h3 id=&#34;毎日の生活&#34;&gt;毎日の生活&lt;/h3&gt;

&lt;p&gt;ベルリンとポツダム（あのポツダム宣言の場所）までは電車で30-60分ほどで行き来することができるため、私はベルリンのKreuzberg(クロイツベルク)に住んでいました。
朝9:00に家を出て10:00頃にHPIにつき、12:30ぐらいになったら &lt;strong&gt;みんなで&lt;/strong&gt; 学食にご飯にいき、遅くて18:00頃までには帰るという文化的で非常に規則正しい毎日でした。
このあたり、マジでドイツらしいです。あまりに遅く残っていると警備の人に「もう閉めるぞ？」と言われたりします。限られし時間の中でいかに自分の研究を進めるかが鍵となってきます。&lt;/p&gt;

&lt;p&gt;また、15:00頃になると「今日ボルダリング行こうぜ？」とか「走りに行こうぜ」などのお誘いが飛ぶため、17:00頃にはラボをすっ飛んで遊びに行くなんてこともザラでした。
ただむしろこういう生活を続けていくと、 &lt;strong&gt;ラボでダラダラ過ごすなんてことがなくなり、かなりテンポよく研究をすすめられたと思います。&lt;/strong&gt; ラボの人と頻繁に遊びに行ったり、 &lt;strong&gt;ドイツ国技であるところのボドゲ&lt;/strong&gt; をやったりすることで、スムーズに仲良くなれました。クッソ楽しかったです。&lt;/p&gt;

&lt;p&gt;ラボMTGは毎週水曜日に1時間きっかりでやります。短いと思う人がいるかと思いますが、普段から先生や周りの学生と話しているのでこれと言って不足しているとは思いませんでした。みんなが発表している間は小さなカードに話を聞いている間のメモを取って、発表している人に最後に渡してあげます。全員が全員アウトプット/FBしてあげるという感じでした。メンバーの殆どがバリバリ実装できるマンだったので、研究の方針から細かい実装まで多方面からアドバイスが貰える有意義な時間でした。&lt;/p&gt;

&lt;p&gt;プロジェクトが始まるとすぐにペーパープロトタイピングとフィジカルプロトタイピングのくり返しで・・・&lt;/p&gt;

&lt;h3 id=&#34;keihigu-教員は共有リソース&#34;&gt;keihigu「教員は共有リソース」&lt;/h3&gt;

&lt;p&gt;そうそう、暦本研10周年のオープンハウスで&lt;a href=&#34;https://twitter.com/keihigu&#34; target=&#34;_blank&#34;&gt;keihigu&lt;/a&gt;さんが「暦本研の秘訣」みたいなテーマのパネルでこう言われていました。
&lt;strong&gt;はっきり言って至言です。すごい。&lt;/strong&gt; 時間が限られている中で、ラボとブレインであるところの教授と、如何にたくさん議論して、如何に自分の実装をFBしてもらって、ブラッシュアップを回しまくるかが鍵だと思います。鉄と同じでぶっ叩いてもらわないと強固にならないので、思い切って見せまくるということが大事だと学びました。ラボによって、教員が学生とあまり時間を過ごさないという声をたまに聞きます。留学して、ラボにいる間は常に議論できる環境づくりができているといいな、という感じでした。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://jotaros.github.io/img/inami.png&#34; alt=&#34;inami&#34; /&gt;
&lt;em&gt;@drinamiの語る共有リソース&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;ラピッドプロトタイプとFBループに関しては、&lt;a href=&#34;https://twitter.com/ken0324&#34; target=&#34;_blank&#34;&gt;@ken0324&lt;/a&gt; さんの図が思い出されました。この図を始めて見たときに「おにぎりを握るようにテンポよく繰り返す」って、結局どれくらいテンポよく繰り返せばいいんだ・・・ってなってましたが、今回取り組んだ内容だと1-3時間に1回とかのレベルでした。一日でかなり進捗を産んだ感があった印象です(ただ、deadlineまで相当時間がなかったのであえてこういう時間の使い方をしていた、というのもあると思います)。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://jotaros.github.io/img/ken.png&#34; alt=&#34;nakagaki&#34; /&gt;
&lt;em&gt;@ken0324 おにぎりの図&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&#34;休日&#34;&gt;休日&lt;/h3&gt;

&lt;p&gt;休みの日はベルリンを中心に観光したり、ごろごろしたりして疲れたアタマを休ませたりしていました。
公園はいつも人で賑わっていたり、博物館や美術館は有名な展示が多いため、常に観光客でごった返しています（直行便がなくアクセスが悪いからか、日本人観光客がミュンヘンなどに比べてかなり少なかった印象です）。色んな所に行きましたが、記事が無限に長くなるのでちょっとだけ。&lt;/p&gt;

&lt;p&gt;世界最大の動物園の一つであるベルリン動物園がめちゃお気に入りでした。上野でパンダの赤ちゃんを見ようとしたら5時間ぐらい待たされそうですが、ベルリンはすんなり入ってすんなり見れました。間近。めちゃかわいい。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://jotaros.github.io/img/berlin-panda.png&#34; alt=&#34;panda&#34; /&gt;
&lt;em&gt;Panda, Zoologischer Garten, Berlin. 笹をむしゃむしゃ食べてるところの後ろ姿。人混みがほぼない。&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://jotaros.github.io/img/berlin-holidays.png&#34; alt=&#34;holidays&#34; /&gt;
&lt;em&gt;Spree, Belrin. よる9時ぐらいまでこれくらいの明るさの時もあり、公園は賑わっている&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&#34;追い込みからsubmitまで&#34;&gt;追い込みからsubmitまで&lt;/h3&gt;

&lt;p&gt;9月に入ってからは実装したシステムを評価実験し、締め切りギリギリまで踏ん張る毎日が進みました。
当然インターン期間中では最もつらい時期でしたが、周りの学生がたくさんアドバイスをしてくれたり、撮影や実験を手伝ってくれたりしたので心折たれることなく、Submitにこぎつけることが出来ました。けしてひとりでやろうとせず、同じような志やコモンセンスをもつ仲間と協力しながら取り組むことでも、割りとテンポよく研究も進んだかな、という感じです。過去にトップカンファに何度も論文を執筆している人から直接アドバイスを貰えるのは貴重な経験でした。&lt;/p&gt;

&lt;p&gt;最後までラボの皆さんに支えられ、ときには私から議論に参加することもありました。特に相談事があるときは近くにいるだけで「・・・なんだけど、どう思う？」と言われることがありました。各プロジェクトで向いている方向がバラバラにも関わらず、一体感のあるラボでとても過ごしやすかったです。&lt;/p&gt;

&lt;p&gt;Submit後は夜遅くまで開いているバーで美味しいビールとご飯をいただき、静かにベルリンの夜を過ごしました。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://jotaros.github.io/img/berlin-night.png&#34; alt=&#34;holidays&#34; /&gt;
&lt;em&gt;Schwarzes Cafe, Belrin.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;所感&#34;&gt;所感&lt;/h2&gt;

&lt;p&gt;今回の留学は、海外経験・研究経験の乏しい自分に気合一発、という意味でも大変意義あるものでしたが、プロジェクトを進める上でどういう風に取り組んでいけばいいか、最終的に研究の形に落とし込む際にどう意識すればよいかという点でたくさん学ぶ点がありました。また日本のラボで上手くいかなかった自分に対して、ベルリンで毎日どういう過ごし方をしていたかを比べることで、今後の研究生活に何かしら有益なノウハウも得られるだろう、という考えもありました。&lt;/p&gt;

&lt;p&gt;よって、研究ノートには普段の進捗の他にも、&lt;strong&gt;留学先で日本よりもどういう所が良いな・意外だ・もっとこうした方がいい・キツい、といったことを、思いついた時点で全て紙に文章にして起こすようにしていました&lt;/strong&gt; 。今となってはかなりいいことだと思いましたし、こうして記事を書く際にも参考になりました。&lt;/p&gt;

&lt;p&gt;なかなか研究が思うように進まずに頭を抱える時期が長く続きましたが、留学に出てからは、とにかく一人でこもって研究しようとせずに、周りの人を巻き込んでいけるような人になろうと思いました。悩んでるときも決して自分だけが問題なんだ、という風に思い込んではいけないなと感じました。どのみち、まだ一人で研究をろくにマネージできる能力の持ち主ではないので、今のうちに色んな人と研究してノウハウを蓄積していきたいなという風に感じました。&lt;/p&gt;

&lt;h2 id=&#34;まとめ&#34;&gt;まとめ&lt;/h2&gt;

&lt;p&gt;2ヶ月半という短い間でしたが、毎日実装&amp;amp;議論というタフな毎日をテンポよく勧めていくことで、とりあえずはフルペーパー提出にこぎつけることが出来ました。これから些細なことで足踏みすることなく、とりあえずラボに来たら研究を進めるために何かしら作業する、を目標に頑張りたいです。最後に、インターン推薦にあたりお世話になった&lt;a href=&#34;https://twitter.com/drinami&#34; target=&#34;_blank&#34;&gt;@drinami&lt;/a&gt;先生と&lt;a href=&#34;https://twitter.com/narumin&#34; target=&#34;_blank&#34;&gt;@narumin&lt;/a&gt;先生に感謝いたします。&lt;/p&gt;

&lt;p&gt;-Jotaro&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Self avatar pseudo-haptics in Virtual Reality</title>
      <link>https://jotaros.github.io/project/pseudo-haptics-vr/</link>
      <pubDate>Tue, 12 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://jotaros.github.io/project/pseudo-haptics-vr/</guid>
      <description>

&lt;p&gt;We propose a system that presents a feeling of resistive force by modifying joint angles of user’s avatar, and an approach to reduce a feeling of discomfort evoked by a conflict between visual and proprioceptive sensations. Pseudo-haptic feedback enables us to provide haptic sensations by making discrepancy between the position of user’s body in the real world and avatar which represents a part of user’s body in an immersive virtual environment without using any complicated devices. However, a larger discrepancy between proprioceptive and visual sensations can cause a feeling of discomfort to a user, which leads to reduce the effectiveness of pseudo-haptic feedback. Also, modifying a displacement of a body part cannot maintain a consistency of whole body parts of a user and an avatar under an immersive virtual environment. To avoid these problems, we proposed a pseudo-haptic approach of modifying a joint angle of avatar. Our experiments showed that modifying multiple joint angles of an avatar’s arm can reduce a feeling of discomfort but still presents a certain intensity of resistive force, compared to changing only a single joint angle. In the demonstration we show some applications of our proposed system using pseudo-haptic feedback system in a conventional VR system.&lt;/p&gt;

&lt;h2 id=&#34;publication&#34;&gt;Publication&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Jotaro Shigeyama&lt;/strong&gt;, Nami Ogawa, Takuji Narumi, Tomohiro Tanikawa and Michitaka Hirose: Presenting resistive force by modifying joint angles of an avatar, VRSJ Jounal Vol.22, No.3, 2017&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Jotaro Shigeyama&lt;/strong&gt;, Nami Ogawa, Takuji Narumi, Tomohiro Tanikawa and Michitaka Hirose: Presenting a pseudo-haptic feedback in immersive VR environment by modifying avatar&amp;rsquo;s joint angle, IEEE World Haptics 2017, June 2017&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Swinging 3D Lamps</title>
      <link>https://jotaros.github.io/project/swinging-3d-lamps/</link>
      <pubDate>Tue, 12 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://jotaros.github.io/project/swinging-3d-lamps/</guid>
      <description>&lt;p&gt;under construction&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>AgIC Workshop Collection installation design</title>
      <link>https://jotaros.github.io/project/workshop_collection/</link>
      <pubDate>Mon, 20 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://jotaros.github.io/project/workshop_collection/</guid>
      <description>&lt;p&gt;&amp;ldquo;Workshop Collection 10&amp;rdquo;, held in Tokyo was full of families and children to experience the latest technologies.
AgIC workshop has exhibited an installation to show what&amp;rsquo;s possible with its product: conductive ink.&lt;/p&gt;

&lt;p&gt;The installation is designed to be one of Japanese ethnic ornament &amp;ldquo;Kazaguruma&amp;rdquo;, a litte windmill. Japanese ethnic pattern &amp;ldquo;Wagara&amp;rdquo; is printed with conductive ink on each mill with chip LED mouted. Sensors on the board detects human passage, and mills rotate as if wind blew on them.&lt;/p&gt;

&lt;p&gt;We showed that flexible circuit design can easily be built and designing product can be also be flexible.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Project-rsses on jotaro shigeyama</title>
    <link>https://jotaros.github.io/project/index.xml</link>
    <description>Recent content in Project-rsses on jotaro shigeyama</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Jotaro Shigeyama</copyright>
    <lastBuildDate>Fri, 28 Aug 2020 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://jotaros.github.io/project/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>SpeechIOForUnity</title>
      <link>https://jotaros.github.io/project/speechioforunity/</link>
      <pubDate>Fri, 28 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://jotaros.github.io/project/speechioforunity/</guid>
      <description>

&lt;p&gt;A ready-to-use Unity plugin for Speech Input/Output using native Speech API of &lt;i&gt;both&lt;/i&gt; Apple macOS and MS Windows.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/HassoPlattnerInstituteHCI/SpeechIOForUnity&#34; target=&#34;_blank&#34;&gt;github project page is here.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Author: Jotaro Shigeyama and Thijs Roumen&lt;/p&gt;

&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Custom command speech input&lt;/li&gt;
&lt;li&gt;Support for various speed / language.&lt;/li&gt;
&lt;li&gt;Async/await-based interaction design.&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-example.cs&#34;&gt;
SpeechOut speechOut = new SpeechOut();
SpeechIn  speechIn  = new SpeechIn(OnRecognized);

void Start(){
    Dialog();
}

async void Dialog(){
    await speechOut.Speak(&amp;quot;Hello!&amp;quot;);
    await speechIn.Listen(new string[] { &amp;quot;Hello&amp;quot;, &amp;quot;Hi&amp;quot;, &amp;quot;Hey&amp;quot; });
    await speechOut.Speak(&amp;quot;How are you doing?&amp;quot;);
    await speechIn.Listen(new string[] { &amp;quot;I&#39;m fine&amp;quot;, &amp;quot;Nah&amp;quot;, &amp;quot;I&#39;m Sick&amp;quot; });
    //...
}

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This project repo contains&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Unity project (this repo, tested with v.2019.3.0a8)&lt;/li&gt;
&lt;li&gt;XCode project NSSpeechForUnity (author: Jotaro Shigeyama)&lt;/li&gt;
&lt;li&gt;Visual Studio project WindowsVoiceProject originally from here (&lt;a href=&#34;https://chadweisshaar.com/blog/2015/07/02/microsoft-speech-for-unity/&#34; target=&#34;_blank&#34;&gt;https://chadweisshaar.com/blog/2015/07/02/microsoft-speech-for-unity/&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Unity project contains pre-built &lt;code&gt;.dll&lt;/code&gt; and &lt;code&gt;.bundle&lt;/code&gt; from above source project.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;installation&#34;&gt;Installation&lt;/h2&gt;

&lt;p&gt;This plugin works and tested on macOS Catalina or above, and Windows 10 (Windows 8 is not supported).&lt;/p&gt;

&lt;h3 id=&#34;os-setup&#34;&gt;OS setup&lt;/h3&gt;

&lt;p&gt;Right now English / Dutch / German / Japanese are supported. You need to install necessary language module from your OS setting.&lt;/p&gt;

&lt;h3 id=&#34;unity&#34;&gt;Unity&lt;/h3&gt;

&lt;p&gt;Just simply grab all Scripts, Plugins (and if Scenes if you need) to your own Unity project file.&lt;/p&gt;

&lt;h3 id=&#34;potential-installation-issue-for-macos&#34;&gt;Potential installation issue for macOS&lt;/h3&gt;

&lt;p&gt;Some macOS users will experience broken speech input due to missing dictation kits: mostly because of bug on Apple. If you encounter some issue on speech input, please try these.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;make sure your native voice command system works (System Preferences &amp;gt; Accessibility &amp;gt; Voice Command, enabling voice command will invoke macOS system voice command input windows.)

&lt;ul&gt;
&lt;li&gt;Go to &amp;ldquo;Preferences &amp;gt; Accessibility &amp;gt; Sound Control &amp;gt; Language&amp;rdquo; and install language pack.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;In case of macOS have some buggy issues: Try switching your OS language to another, then try to install your desired voice command module (macOS will prompt you to download missing dictation model.)&lt;/li&gt;
&lt;li&gt;Make sure your macOS is the latest version&lt;/li&gt;
&lt;li&gt;Try rebooting the system.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;dev-installation&#34;&gt;dev-Installation&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;XCode (mac)&lt;/li&gt;
&lt;li&gt;Visual Studio (win, latest Windows SDK required)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;testing-your-nsspeechrecognizer&#34;&gt;Testing your NSSpeechRecognizer&lt;/h3&gt;

&lt;p&gt;tested on Apple Swift version 5.1.3&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd NSSpeechTest
swift run
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;using-in-your-own-unity-package&#34;&gt;Using in your own Unity package&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Build NSSpeechForUnity in XCode.&lt;/li&gt;
&lt;li&gt;Copy generated &lt;code&gt;.bundle&lt;/code&gt; file in &lt;code&gt;Assets/Plugins&lt;/code&gt; of your Unity project.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;modifying-voice-command-dictionary&#34;&gt;Modifying voice command dictionary&lt;/h3&gt;

&lt;p&gt;See &lt;code&gt;MyMacSpeechScript.cs&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;documentation&#34;&gt;Documentation&lt;/h2&gt;

&lt;p&gt;coming soon.&lt;/p&gt;

&lt;p&gt;Your feedback is welcome!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kerf-Canceling Mechanisms: Making Laser-Cut Mechanisms Operate Across Different Laser Cutters</title>
      <link>https://jotaros.github.io/project/kerfc/</link>
      <pubDate>Thu, 20 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://jotaros.github.io/project/kerfc/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;https://jotaros.github.io/img/kerfc.png&#34; alt=&#34;Example image&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br&gt;
&lt;center&gt; Kerf-Canceling Mechanisms: Making Laser-Cut Mechanisms Operate Across Different Laser Cutters&lt;/center&gt;&lt;/p&gt;

&lt;h4 id=&#34;abstract&#34;&gt;Abstract&lt;/h4&gt;

&lt;p&gt;Getting laser-cut mechanisms, such as those in microscopes,robots, vehicles, etc., to work, requires all their components to be dimensioned precisely. This precision, however, tends to be lost when fabricating on a different laser cutter, as it is likely to remove more or less material (aka kerf). We address this with what we call kerf-canceling mechanisms. Kerf-canceling mechanisms replace laser-cut bearings, sliders, gear pairs, etc. Unlike their traditional counterparts, however, they keep working when manufactured on a different laser cutter and/or with different kerf. Kerf-canceling mechanisms achieve this by adding an additional wedge element per mechanism. We have created a software tool Kerf-Canceler that locates traditional mechanisms in cutting plans and replaces them with their kerf-canceling counterparts.&lt;/p&gt;

&lt;p&gt;For more details, paper and video, visit &lt;a href=&#34;https://hpi.de/baudisch/projects/kerf-canceling-mechanisms.html&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;UIST2020 Technical Paper&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Kerf-Canceling Mechanisms: Making Laser-Cut Mechanisms Operate Across Different Laser Cutters&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>springFit: Joints and Mounts that Fabricate on Any Laser-Cutter</title>
      <link>https://jotaros.github.io/project/springfit/</link>
      <pubDate>Thu, 01 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://jotaros.github.io/project/springfit/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;https://jotaros.github.io/img/springfit.png&#34; alt=&#34;springfit&#34; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;abstract&#34;&gt;Abstract&lt;/h3&gt;

&lt;p&gt;Joints are crucial to laser cutting as they allow making three-dimensional objects; mounts are crucial because they allow embedding technical components, such as motors. Unfortunately, mounts and joints tend to fail when trying to fabricate a model on a different laser cutter or from a different material. The reason for this lies in the way mounts and joints hold objects in place, which is by forcing them into slightly smaller openings. Such “press fit” mechanisms unfortunately are susceptible to the small changes in diameter that occur when switching to a machine that removes more or less material (“kerf”), as well as to changes in stiffness, as they occur when switching to a different material.&lt;/p&gt;

&lt;p&gt;We present a software tool called springFit that resolves this problem by replacing the problematic press fit-based mounts and joints with what we call cantileverbased mounts and joints. A cantilever spring is simply a long thin piece of material that pushes against the object to be held. Unlike press fits, cantilever springs are robust against variations in kerf and material; they can even handle very high variations, simply by using longer springs. SpringFit converts models in the form of 2D cutting plans by replacing all contained mounts, notch joints, finger joints, and t-joints.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;jotaro-s-contribution&#34;&gt;Jotaro&amp;rsquo;s contribution&lt;/h3&gt;

&lt;p&gt;In this project, Jotaro has contributed to the system that automatically generates ideal parameters of the spring geometry: cantilver spring that produces aimed forces and deflection and will not break easily. Based on the &lt;em&gt;Euler-Bernoulli cantilver theory&lt;/em&gt;, Jotaro has build C++ executable system that optimizes the spring geometry based on given geometric constraints.&lt;/p&gt;

&lt;p&gt;For more details, paper and video, visit &lt;a href=&#34;https://hpi.de/baudisch/projects/springfit.html&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;publication&#34;&gt;Publication&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Thijs Roumen, &lt;u&gt;&lt;strong&gt;Jotaro Shigeyama&lt;/strong&gt;&lt;/u&gt;, Julius Romeo Cosmo Rudolph, Felix Grzelka, and Patrick Baudisch: SpringFit: Joints and Mounts that Fabricate on Any Laser Cutter, In Proceedings of UIST&amp;rsquo;19, October, 2019, New Orleans, LA, US.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Project Transcalibur</title>
      <link>https://jotaros.github.io/project/transcalibur/</link>
      <pubDate>Fri, 01 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://jotaros.github.io/project/transcalibur/</guid>
      <description>

&lt;h3 id=&#34;downloads&#34;&gt;Downloads:&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#39;https://jotaros.github.io/transcalibur/shigeyama-preprint.pdf&#39;&gt;Preprint Paper (PDF 7MB)&lt;/a&gt;  //   &lt;a href=&#39;https://jotaros.github.io/transcalibur/transcalibur-movie.mp4&#39;&gt; Movie (MP3 20MB)&lt;/a&gt;   //     &lt;a href=&#39;https://jotaros.github.io/transcalibur/presskit.zip&#39;&gt; Press Kit (ZIP Archive 20MB)&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;


&lt;div style=&#34;position: relative; padding-bottom: 56.25%; padding-top: 30px; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;//www.youtube.com/embed/OiSbn6D5kwA?&#34; 
  style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%;&#34; allowfullscreen frameborder=&#34;0&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br&gt;
&lt;center&gt; Transcalibur: A Weight Shifting Virtual Reality Controller&lt;br&gt; for 2D Shape Rendering based on Computational Perception Model&lt;/center&gt;&lt;/p&gt;

&lt;h4 id=&#34;abstract&#34;&gt;Abstract&lt;/h4&gt;

&lt;p&gt;Humans can estimate the shape of a wielded object through the illusory feeling of the mass properties of the object obtained using their hands. Even though the shape of hand-held objects influences immersion and realism in virtual reality (VR), it is difficult to design VR controllers for rendering desired shapes according to the perceptions derived from the illusory effects of mass properties and shape perception. We propose Transcalibur, which is a hand-held VR controller that can render a 2D shape by changing its mass properties on a 2D planar area. We built a computational perception model using a data-driven approach from the collected data pairs of mass properties and perceived shapes. This enables Transcalibur to easily and effectively provide convincing shape perception based on complex illusory effects. Our user study showed that the system succeeded in providing the perception of various desired shapes in a virtual environment.&lt;/p&gt;

&lt;h4 id=&#34;award&#34;&gt;Award:&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#39;https://jotaros.github.io/img/hm.png&#39; style=&#39;width:30px; min-height:30px; margin-bottom:0px;&#39;&gt; This paper has been awarded a Best Paper Honorable Mention at CHI2019.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&#39;https://jotaros.github.io/img/transcalibur-teaser.png&#39; width=&#39;100%&#39;&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&#39;https://jotaros.github.io/img/transcalibur-neu.jpg&#39; width=&#39;100%&#39;&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&#39;https://jotaros.github.io/img/transcalibur-neu-zwei.JPG&#39; width=&#39;100%&#39;&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&#34;hardware&#34;&gt;Hardware&lt;/h4&gt;

&lt;p&gt;The weight moving mechanism is designed to move along the 2D planner space and to be &lt;em&gt;non-backdrivable&lt;/em&gt;. The &lt;em&gt;angular mechanism&lt;/em&gt; enables to rotate two arms, and &lt;em&gt;weight mechanisms&lt;/em&gt; enable to shift the position of the weight module independently.&lt;/p&gt;

&lt;p&gt;&lt;center&gt; &lt;img class=&#39;half&#39; src=&#39;https://jotaros.github.io/img/transform.gif&#39;&gt; &lt;img class=&#39;half&#39; src=&#39;https://jotaros.github.io/img/transcalibur-hardware.png&#39;&gt;&lt;/center&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&#34;computational-perception-model&#34;&gt;Computational Perception Model&lt;/h4&gt;

&lt;p&gt;Based on the object shown in VR, the shape of the controller that users grasp is dynamically changed so that its shape perception matches the target object.
In our system, we create a computational model that maps mass properties to haptically perceived shape using a data-driven approach.
We correct the perceived shape data of the VR controller with different mass properties through a perceptual experiment and map these data using regression.
From the model, we determine the mass properties of the VR controller that optimizes the perceived shape of the controller to be the target object shown in virtual environment (VE).
In this manner, we can easily and efficiently render an arbitrary 2D shape through the controller.&lt;/p&gt;

&lt;p&gt;We assumed a perception model $f$ that maps the physical configuration of the controller $\phi$ to the perceived shape of the wielded object in VR $\psi$:&lt;/p&gt;

&lt;p&gt;$$
f: \phi \mapsto \psi
$$&lt;/p&gt;

&lt;p&gt;&lt;img src=&#39;https://jotaros.github.io/img/transcalibur-approach.png&#39;&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&#34;experiments-results&#34;&gt;Experiments / Results&lt;/h4&gt;

&lt;p&gt;In the data collection experiment, we provided the participants with various shapes of the controller and asked them to report the perceived shapes in VE.
This generates matched pairs of $(\phi_i,\psi_i)$, which are used to build a regression model for the training data. Using the obtained data pairs, we performed regression analysis to build a map $f$ from the configurations of the controller onto the perceived shapes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#39;https://jotaros.github.io/img/dataCollection.jpg&#39;&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;To measure the validity of the perception model, we conducted validation experiments. The ten virtual shapes were manually determined such that variations in height, width, symmetricity, and asymmetricity of the target shapes could be evaluated. Overall, our perception model succeeded in providing various target shapes in VR for Transcalibur, leaving a few shapes with confusions on shapes 4 and 5 or 8 and 9.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#39;https://jotaros.github.io/img/transcalibur-outputlist.png&#39;&gt;&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;img class=&#39;half&#39; src=&#39;https://jotaros.github.io/img/transcalibur-confusionmat.png&#39;&gt;&lt;/center&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h4&gt;

&lt;p&gt;In this paper, we introduced Transcalibur: the weight moving VR controller for 2D haptic shape illusion.
We implemented a hardware prototype, which can change its mass property in 2D planar space, and applied data-driven methods to obtain maps between mass property and perceived shape.
Based on the demonstration and experiment, we succeeded in rendering various shape perceptions through the controller based on pre-computed perception model.
As a future work, we further investigate details on time factor of shape changing in VR, and we aim to develop a simpler design and yet maximizes range of rendering shape.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;CHI2019 Technical Paper&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Transcalibur: A Weight Shifting Virtual Reality Controller for 2D Shape Rendering based on Computational Perception Model&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;h4 id=&#34;authors&#34;&gt;Authors:&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://jotaros.github.io&#34; target=&#34;_blank&#34;&gt;Jotaro Shigeyama&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://takeruace.github.io/&#34; target=&#34;_blank&#34;&gt;Takeru Hashimoto&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.shigeodayo.com&#34; target=&#34;_blank&#34;&gt;Shigeo Yoshida&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cyber.t.u-tokyo.ac.jp/~narumi/&#34; target=&#34;_blank&#34;&gt;Takuji Narumi&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cyber.t.u-tokyo.ac.jp/&#34; target=&#34;_blank&#34;&gt;Tomohiro Tanikawa&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;and &lt;a href=&#34;https://twitter.com/_anohito&#34; target=&#34;_blank&#34;&gt;Michitaka Hirose&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;downloads-1&#34;&gt;Downloads:&lt;/h4&gt;

&lt;p&gt;&lt;a href=&#39;https://jotaros.github.io/transcalibur/shigeyama-preprint.pdf&#39;&gt;Preprint Paper (PDF 7MB)&lt;/a&gt; -//-  &lt;a href=&#39;https://jotaros.github.io/transcalibur/transcalibur-movie.mp4&#39;&gt; Movie (MP3 20MB)&lt;/a&gt;  -//-    &lt;a href=&#39;https://jotaros.github.io/transcalibur/presskit.zip&#39;&gt; Press Kit (ZIP Archive 20MB)&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&#34;archive&#34;&gt;Archive:&lt;/h4&gt;

&lt;p&gt;Previously presented SIGGRAPH2018 E-tech Promotion can be seen &lt;a href=&#34;http://www.cyber.t.u-tokyo.ac.jp/~jotaro/transcalibur_web/&#34; target=&#34;_blank&#34;&gt;here.&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>DualPanto: A Haptic Device that Enables Blind Users to Continuously Interact with Virtual Worlds</title>
      <link>https://jotaros.github.io/project/dualpanto/</link>
      <pubDate>Mon, 20 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>https://jotaros.github.io/project/dualpanto/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;https://jotaros.github.io/img/dualpanto.png&#34; alt=&#34;Pantograph&#34; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;We present a new haptic device that enables blind users to continuously track the absolute position of moving objects in spatial virtual environments, as is the case in sports or shooter games. Users interact with DualPanto by operating the me handle with one hand and by holding on to the it handle with the other hand.&lt;/p&gt;

&lt;p&gt;Each handle is connected to a pantograph haptic input/output device. The key feature is that the two handles are spatially registered with respect to each other. When guiding their avatar through a virtual world using the me handle, spatial registration enables users to track moving objects by having the device guide the output hand.&lt;/p&gt;

&lt;p&gt;This allows blind players of a 1-on-1 soccer game to race for the ball or evade an opponent; it allows blind players of a shooter game to aim at an opponent and dodge shots. In our user study, blind participants reported very high enjoyment when using the device to play (6.&lt;sup&gt;5&lt;/sup&gt;&amp;frasl;&lt;sub&gt;7&lt;/sub&gt;).&lt;/p&gt;

&lt;p&gt;For more details visit &lt;a href=&#34;https://hpi.de/baudisch/projects/dualpanto.html&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;publication&#34;&gt;Publication&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Oliver Schneider, &lt;u&gt;&lt;strong&gt;Jotaro Shigeyama&lt;/strong&gt;&lt;/u&gt;, Robert Kovacs, Thijs Jan Roumen, Sebastian Marwecki, Nico Boeckhoff, Patrick Baudisch, DualPanto: A Haptic Device that Enables Blind Users to Continuously Interact with Virtual Worlds,
In Proceedings of UIST&amp;rsquo;18, October, &lt;strong&gt;&lt;em&gt;TO APPEAR&lt;/em&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Self avatar pseudo-haptics in Virtual Reality</title>
      <link>https://jotaros.github.io/project/pseudo-haptics-vr/</link>
      <pubDate>Tue, 12 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://jotaros.github.io/project/pseudo-haptics-vr/</guid>
      <description>

&lt;p&gt;We propose a system that presents a feeling of resistive force by modifying joint angles of user’s avatar, and an approach to reduce a feeling of discomfort evoked by a conflict between visual and proprioceptive sensations. Pseudo-haptic feedback enables us to provide haptic sensations by making discrepancy between the position of user’s body in the real world and avatar which represents a part of user’s body in an immersive virtual environment without using any complicated devices. However, a larger discrepancy between proprioceptive and visual sensations can cause a feeling of discomfort to a user, which leads to reduce the effectiveness of pseudo-haptic feedback. Also, modifying a displacement of a body part cannot maintain a consistency of whole body parts of a user and an avatar under an immersive virtual environment. To avoid these problems, we proposed a pseudo-haptic approach of modifying a joint angle of avatar. Our experiments showed that modifying multiple joint angles of an avatar’s arm can reduce a feeling of discomfort but still presents a certain intensity of resistive force, compared to changing only a single joint angle. In the demonstration we show some applications of our proposed system using pseudo-haptic feedback system in a conventional VR system.&lt;/p&gt;

&lt;h2 id=&#34;publication&#34;&gt;Publication&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Jotaro Shigeyama&lt;/strong&gt;, Nami Ogawa, Takuji Narumi, Tomohiro Tanikawa and Michitaka Hirose: Presenting resistive force by modifying joint angles of an avatar, VRSJ Jounal Vol.22, No.3, 2017&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Jotaro Shigeyama&lt;/strong&gt;, Nami Ogawa, Takuji Narumi, Tomohiro Tanikawa and Michitaka Hirose: Presenting a pseudo-haptic feedback in immersive VR environment by modifying avatar&amp;rsquo;s joint angle, IEEE World Haptics 2017, June 2017&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Swinging 3D Lamps</title>
      <link>https://jotaros.github.io/project/swinging-3d-lamps/</link>
      <pubDate>Tue, 12 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://jotaros.github.io/project/swinging-3d-lamps/</guid>
      <description>&lt;p&gt;under construction&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>AgIC Workshop Collection installation design</title>
      <link>https://jotaros.github.io/project/workshop_collection/</link>
      <pubDate>Mon, 20 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://jotaros.github.io/project/workshop_collection/</guid>
      <description>&lt;p&gt;&amp;ldquo;Workshop Collection 10&amp;rdquo;, held in Tokyo was full of families and children to experience the latest technologies.
AgIC workshop has exhibited an installation to show what&amp;rsquo;s possible with its product: conductive ink.&lt;/p&gt;

&lt;p&gt;The installation is designed to be one of Japanese ethnic ornament &amp;ldquo;Kazaguruma&amp;rdquo;, a litte windmill. Japanese ethnic pattern &amp;ldquo;Wagara&amp;rdquo; is printed with conductive ink on each mill with chip LED mouted. Sensors on the board detects human passage, and mills rotate as if wind blew on them.&lt;/p&gt;

&lt;p&gt;We showed that flexible circuit design can easily be built and designing product can be also be flexible.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>JTRWatch</title>
      <link>https://jotaros.github.io/project/jtrwatch/</link>
      <pubDate>Mon, 20 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://jotaros.github.io/project/jtrwatch/</guid>
      <description>&lt;p&gt;JTRWatch is a prototype of smartwatch using Intel Edison, built in 2 weeks during a school project.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lighting Coaster:AgIC Workshop design</title>
      <link>https://jotaros.github.io/project/agic_workshop/</link>
      <pubDate>Mon, 20 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://jotaros.github.io/project/agic_workshop/</guid>
      <description>&lt;p&gt;We organized a new program of workshop where we made lightning coaster, using Circuit Sticker from Chibitronics.
AgIC got a 2nd prize in Global Fab Award in the event.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Project CoPRO</title>
      <link>https://jotaros.github.io/project/copro/</link>
      <pubDate>Wed, 01 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>https://jotaros.github.io/project/copro/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Dokimakura:Hugging pillow with realistic haptic feedback</title>
      <link>https://jotaros.github.io/project/dokimakura/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>https://jotaros.github.io/project/dokimakura/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;https://jotaros.github.io/img/farmbot-hug.png&#34; alt=&#34;Example image&#34; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&amp;ldquo;Dokimakura&amp;rdquo; is the hugging pillow with realistic heartbeat haptic feedback through &amp;ldquo;soinet&amp;rdquo;:heartbeat SNS platform.
This project is awarded the best prize in the hackathon &amp;ldquo;HackU&amp;rdquo; by Yahoo! Japan inc.&lt;/p&gt;

&lt;p&gt;Dokimakura is designed to show augmentation of &amp;lsquo;presence&amp;rsquo;, feeling of &amp;ldquo;someone&amp;rsquo;s there&amp;rdquo;, of especially beloved one.
Loneliness is a major problem of modern stressful city. We thought that act of hug, feeling of warmth and life can reduce the feeling of loneliness.&lt;/p&gt;

&lt;p&gt;Hugging pillow is a metaphor of physical presence, and we tried to connect it to the social network. User voice obtained from demonstration:&amp;ldquo;Feeling of hugging some animal&amp;rdquo; &amp;ldquo;Realistic haptic feedback felt as if actually hearing someone&amp;rsquo;s heartbeat&amp;rdquo; showed that the system does presence augmentation.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;award&#34;&gt;Award&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Yahoo! HackU 東京大学x多摩美術大学 グランプリ&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Farmbot</title>
      <link>https://jotaros.github.io/project/farmbot/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>https://jotaros.github.io/project/farmbot/</guid>
      <description>&lt;p&gt;Farmbot is a opensource agricultural robot project by
&lt;a href=&#34;https://farmbot.io/&#34; target=&#34;_blank&#34;&gt;farmbot.io&lt;/a&gt;.
First working copy of the project was built for the exhibition in Tsuruoka Art Forum, during the exhibition
&lt;a href=&#34;http://agri-revolution3.com/&#34; target=&#34;_blank&#34;&gt;Agricultural Revolution 3.0&lt;/a&gt; organized by &lt;a href=&#34;http://o-n-inc.com/index.html&#34; target=&#34;_blank&#34;&gt;Object of null,Inc.&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Remote aerobotics project</title>
      <link>https://jotaros.github.io/project/plane/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>https://jotaros.github.io/project/plane/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>